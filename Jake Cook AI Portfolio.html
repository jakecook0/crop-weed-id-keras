<!DOCTYPE html>
<!-- saved from url=(0069)https://usu.instructure.com/courses/639034/pages/jake-cooks-portfolio -->
<html class="" dir="ltr" lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <script src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/loader.jsp" async=""
    type="text/javascript"></script>
  <script type="text/javascript" async=""
    src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/analytics.js"></script>
  <script src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/launch.js" async=""
    type="text/javascript"></script>
  <script async="" src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/analytics.js"></script>
  <script async="" src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/gtm.js"></script>

  <link rel="preconnect" href="https://fonts.gstatic.com/" crossorigin="">
  <link rel="stylesheet" media="screen"
    href="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/lato_extended-a29d3d859f.css">
  <script>if (navigator.userAgent.match(/(MSIE|Trident\/)/)) location.replace('/ie-is-not-supported.html')</script>
  <script
    data-dapp-detection="">!function () { let e = !1; function n() { if (!e) { const n = document.createElement("meta"); n.name = "dapp-detected", document.head.appendChild(n), e = !0 } } if (window.hasOwnProperty("ethereum")) { if (window.__disableDappDetectionInsertion = !0, void 0 === window.ethereum) return; n() } else { var t = window.ethereum; Object.defineProperty(window, "ethereum", { configurable: !0, enumerable: !1, set: function (e) { window.__disableDappDetectionInsertion || n(), t = e }, get: function () { if (!window.__disableDappDetectionInsertion) { const e = arguments.callee; e && e.caller && e.caller.toString && -1 !== e.caller.toString().indexOf("getOwnPropertyNames") || n() } return t } }) } }();</script>

  <link rel="shortcut icon" type="image/x-icon"
    href="https://du11hjcvx0uqb.cloudfront.net/br/dist/images/favicon-e10d657a73.ico">
  <link rel="apple-touch-icon"
    href="https://du11hjcvx0uqb.cloudfront.net/br/dist/images/apple-touch-icon-585e5d997d.png">
  <link rel="stylesheet" media="all"
    href="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/variables-8391c84da435c9cfceea2b2b3317ff66.css">
  <link rel="stylesheet" media="all"
    href="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/common-03cbb0ce89.css">
  <meta name="apple-itunes-app" content="app-id=480883488">
  <link rel="manifest" href="https://usu.instructure.com/web-app-manifest/manifest.json">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="theme-color" content="#003366">
  <link rel="stylesheet" media="all"
    href="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/wiki_page-532fb0ad99.css">
  <link rel="stylesheet" media="all"
    href="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/production_upload_canvas_global.css">
  <script>
    function _earlyClick(e) {
      var c = e.target
      while (c && c.ownerDocument) {
        if (c.getAttribute('href') == '#' || c.getAttribute('data-method')) {
          if (!e.defaultPrevented) {
            e.preventDefault()
          }
          (_earlyClick.clicks = _earlyClick.clicks || []).push(c)
          break
        }
        c = c.parentNode
      }
    }
    document.addEventListener('click', _earlyClick)
  </script>


  <script>
    INST = { "environment": "production", "allowMediaComments": true, "kalturaSettings": { "domain": "nv.instructuremedia.com", "resource_domain": "nv.instructuremedia.com", "rtmp_domain": "iad.rtmp.instructuremedia.com", "partner_id": "9", "subpartner_id": "0", "player_ui_conf": "0", "kcw_ui_conf": "0", "upload_ui_conf": "0", "max_file_size_bytes": 534773760, "do_analytics": false, "hide_rte_button": false, "js_uploader": true }, "logPageViews": true, "maxVisibleEditorButtons": 3, "editorButtons": [{ "name": "Commons Favorites", "id": 17401, "favorite": false, "url": "https://lor.instructure.com/api/lti/favorite-resources", "icon_url": "https://lor.instructure.com/img/icon_commons.png", "canvas_icon_class": null, "width": 800, "height": 400, "use_tray": true, "description": "\u003cp\u003eFind and share course content\u003c/p\u003e\n" }, { "name": "eMedia", "id": 18712, "favorite": false, "url": "https://eq.uen.org/emedia/canvassignon.do?action=selectOrAdd\u0026cancelDisabled=true", "icon_url": "https://eq.uen.org/emedia/images/equella.gif", "canvas_icon_class": null, "width": 1024, "height": 768, "use_tray": false, "description": "\u003cp\u003eLearning Resources from eMedia\u003c/p\u003e\n" }, { "name": "Upload / Embed Image", "id": 19677, "favorite": true, "url": "https://uei.ciditools.com/", "icon_url": "https://uei.ciditools.com/editorIcon.png", "canvas_icon_class": null, "width": 800, "height": 600, "use_tray": false, "description": "\u003cp\u003eCrop and upload images to Canvas\u003c/p\u003e\n" }, { "name": "Embed Kaltura Media", "id": 21083, "favorite": true, "url": "https://1530551-1.kaf.kaltura.com/browseandembed/index/browseandembed", "icon_url": "https://cdnsecakmi.kaltura.com/content/uiconf/canvas/kaltura_sun.png", "canvas_icon_class": null, "width": 1100, "height": 600, "use_tray": false, "description": "\u003cp\u003eEmbed Media Button\u003c/p\u003e\n" }, { "name": "Niche Academy", "id": 30933, "favorite": false, "url": "https://my.nicheacademy.com/api/launch-lti", "icon_url": "https://my.nicheacademy.com/assets/images/favicon.ico", "canvas_icon_class": null, "width": 600, "height": 600, "use_tray": false, "description": "\u003cp\u003eWe deliver immediately accessible online training. We do this with less overhead for learners, instructors, and administrators than you'll find with any other learning platform on the planet.\u003c/p\u003e\n" }, { "name": "Google Drive (LTI 1.3)", "id": 35258, "favorite": false, "url": "https://assignments.google.com/lti/e", "icon_url": "https://www.gstatic.com/images/branding/product/1x/drive_16dp.png", "canvas_icon_class": null, "width": 690, "height": 530, "use_tray": false, "description": "\u003cp\u003eCollect, analyze, and grade student work with Google Assignments\u003c/p\u003e\n" }, { "name": "Insert a Wiley Resource", "id": 39611, "favorite": false, "url": "https://lti.education.wiley.com/wpng/api/v1/lti/resourcediscoverytool", "icon_url": "https://education.wiley.com/assets/images/wp.svg", "canvas_icon_class": null, "width": 1160, "height": 660, "use_tray": false, "description": "\u003cp\u003eOutcomes-based learning.\u003c/p\u003e\n" }, { "name": "Google Apps", "id": 44046, "favorite": false, "url": "https://google-drive-lti-iad-prod.instructure.com/lti/rce-content-selection", "icon_url": "https://google-drive-lti-iad-prod.instructure.com/icon.png", "canvas_icon_class": null, "width": 700, "height": 600, "use_tray": false, "description": "\u003cp\u003eAllows you to pull in documents from Google Drive to Canvas\u003c/p\u003e\n" }] };
    ENV = { "ASSET_HOST": "https://du11hjcvx0uqb.cloudfront.net/br", "active_brand_config_json_url": "https://du11hjcvx0uqb.cloudfront.net/br/dist/brandable_css/61451b4c24553b78ff8eb71dd6ee3b6a/variables-8391c84da435c9cfceea2b2b3317ff66.json", "url_to_what_gets_loaded_inside_the_tinymce_editor_css": ["https://du11hjcvx0uqb.cloudfront.net/br/dist/brandable_css/61451b4c24553b78ff8eb71dd6ee3b6a/variables-8391c84da435c9cfceea2b2b3317ff66.css", "https://du11hjcvx0uqb.cloudfront.net/br/dist/brandable_css/responsive_layout_normal_contrast/bundles/what_gets_loaded_inside_the_tinymce_editor-8bf5101009.css", "https://du11hjcvx0uqb.cloudfront.net/br/dist/brandable_css/no_variables/bundles/lato_extended-a29d3d859f.css"], "url_for_high_contrast_tinymce_editor_css": ["https://du11hjcvx0uqb.cloudfront.net/br/dist/brandable_css/default/variables-high_contrast-8391c84da435c9cfceea2b2b3317ff66.css", "https://du11hjcvx0uqb.cloudfront.net/br/dist/brandable_css/responsive_layout_high_contrast/bundles/what_gets_loaded_inside_the_tinymce_editor-9bad98d5d7.css", "https://du11hjcvx0uqb.cloudfront.net/br/dist/brandable_css/no_variables/bundles/lato_extended-a29d3d859f.css"], "current_user_id": "1298792", "current_user_roles": ["user", "student"], "current_user_types": [], "current_user_disabled_inbox": false, "files_domain": "cluster14.canvas-user-content.com", "DOMAIN_ROOT_ACCOUNT_ID": "10090000000000015", "k12": false, "use_responsive_layout": true, "use_rce_enhancements": true, "rce_auto_save": true, "help_link_name": "Help", "help_link_icon": "help", "use_high_contrast": false, "auto_show_cc": false, "disable_celebrations": false, "disable_keyboard_shortcuts": false, "LTI_LAUNCH_FRAME_ALLOWANCES": ["geolocation *", "microphone *", "camera *", "midi *", "encrypted-media *", "autoplay *"], "DEEP_LINKING_POST_MESSAGE_ORIGIN": "https://usu.instructure.com", "DEEP_LINKING_LOGGING": null, "SETTINGS": { "open_registration": true, "collapse_global_nav": false, "show_feedback_link": true }, "DIRECT_SHARE_ENABLED": false, "FEATURES": { "cc_in_rce_video_tray": true, "featured_help_links": true, "rce_pretty_html_editor": true, "rce_better_file_downloading": true, "rce_better_file_previewing": true, "expand_cc_languages": false, "responsive_awareness": true, "responsive_misc": true, "product_tours": false, "module_dnd": true, "files_dnd": true, "unpublished_courses": true, "usage_rights_discussion_topics": true, "inline_math_everywhere": true, "granular_permissions_manage_users": true, "canvas_for_elementary": false, "canvas_k6_theme": false, "new_math_equation_handling": true }, "current_user": { "id": "1298792", "display_name": "Jake Cook", "avatar_image_url": "https://secure.gravatar.com/avatar/e86b94dd65e9eb0f03ecfd3c20c166af?s=128\u0026d=identicon", "html_url": "https://usu.instructure.com/about/1298792", "pronouns": null, "avatar_is_fallback": false }, "page_view_update_url": "/page_views/61ae14f5-dbd3-44dd-a25e-0ac9398be269?page_view_token=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpIjoiNjFhZTE0ZjUtZGJkMy00NGRkLWEyNWUtMGFjOTM5OGJlMjY5IiwidSI6MTAwOTAwMDAwMDEyOTg3OTIsImMiOiIyMDIxLTA1LTA1VDA4OjMxOjE3LjYxWiJ9.eMckPC0q2X890fqQ_qdtjPj_NSnSBtuK9CNCEqTx-3Q", "context_asset_string": "course_639034", "ping_url": "https://usu.instructure.com/api/v1/courses/639034/ping", "TIMEZONE": "America/Denver", "CONTEXT_TIMEZONE": "America/Denver", "LOCALE": "en", "BIGEASY_LOCALE": "en_US", "FULLCALENDAR_LOCALE": "en", "MOMENT_LOCALE": "en", "rce_auto_save_max_age_ms": 86400000, "HOMEROOM_COURSE": false, "K5_MODE": false, "WIKI_RIGHTS": { "read": true }, "PAGE_RIGHTS": { "read": true, "update_content": true, "read_revisions": true }, "DEFAULT_EDITING_ROLES": null, "WIKI_PAGES_PATH": "/courses/639034/pages", "WIKI_PAGE": { "title": "Jake Cook's Portfolio", "created_at": "2021-02-07T11:13:50-07:00", "url": "jake-cooks-portfolio", "editing_roles": "teachers,students", "page_id": "1590446", "last_edited_by": { "id": "1298792", "display_name": "Jake Cook", "avatar_image_url": "https://secure.gravatar.com/avatar/e86b94dd65e9eb0f03ecfd3c20c166af?s=128\u0026d=identicon", "html_url": "https://usu.instructure.com/courses/639034/users/1298792", "pronouns": null }, "published": true, "hide_from_students": false, "front_page": false, "html_url": "https://usu.instructure.com/courses/639034/pages/jake-cooks-portfolio", "todo_date": null, "updated_at": "2021-05-05T02:31:16-06:00", "locked_for_user": false, "body": "\u003cp\u003e\u003cspan style=\"font-size: 24px;\"\u003e\u003cstrong\u003ePlant Classification with Neural Networks\u003c/strong\u003e\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"font-size: 14pt;\"\u003e\u003cspan style=\"text-decoration: underline;\"\u003eCompleted Objectives:\u003c/span\u003e\u003c/span\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cspan style=\"font-size: 12pt;\"\u003eRunning notebook with existing keras CNN (97%+ accuracy): \u003ca class=\"inline_disabled\" href=\"https://www.kaggle.com/limitpointinf0/crop-vs-weeds\" target=\"_blank\"\u003ehttps://www.kaggle.com/limitpointinf0/crop-vs-weeds\u003c/a\u003e\u003c/span\u003e\u003c/li\u003e\n\u003cli\u003e\u003cspan style=\"font-size: 12pt;\"\u003eSaving and loading trained models\u003c/span\u003e\u003c/li\u003e\n\u003cli\u003e\u003cspan style=\"font-size: 12pt;\"\u003eTesting prediction capability\u003c/span\u003e\u003c/li\u003e\n\u003cli\u003e\u003cspan style=\"font-size: 12pt;\"\u003eTraining visuals\u003c/span\u003e\u003c/li\u003e\n\u003cli\u003eIntegrating additional data -- continuous effort\u003c/li\u003e\n\u003cli\u003eConfusion matrices\u003c/li\u003e\n\u003cli\u003eMulti-model comparisons for overfitting determination\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cspan style=\"text-decoration: underline;\"\u003e\u003cstrong\u003e\u003cspan style=\"font-size: 18pt;\"\u003eMotivations\u003c/span\u003e\u003c/strong\u003e\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003eThis project aims to understand and reduce carbon emissions in one sector of one market which contributes moderately to global warming: Agricultural pesticide usage. Agriculture contributes over 14% of all greenhouse gas emissions produced globally, where 2% of the global emissions is due directly to nitrogen fertilizer synthesizing and use and other agri-chemicals. Additionally, pesticides are in heavy use to reduce crop competition with local or invasive species. The combination of these items increases soil and ecosystem degradation, and leeches chemicals into waterways and local communities.\u003c/p\u003e\n\u003cp\u003eThese problems are rectifiable with adoption of precision agriculture, which meshes modern technology, farmers, and plants for optimal and efficient use of lands, materials, and time to increase yield while decreasing costs. In particular, the pesticide issue can be solved directly by integrating a machine intelligence model with a mechanical weeding or a precision spot sprayer component to minimize or eliminate pesticide chemical use. In addition, reducing the soil load by removing competing plants potentially allows both higher yields and fewer soil maintenance steps such as nitrogen injection, particularly when coupled with smart crop rotation and other sustainable farming practices.\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"text-decoration: underline; font-size: 18pt;\"\u003e\u003cstrong\u003eProblem\u003c/strong\u003e\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"font-size: 12pt;\"\u003eFor an image of a small plot of cropland (no larger than a few feet on a side), identify the crops, in seedling growth stage, and differentiate from weeds which may or may not be present in each image. Determine the positions of the weeds to be mechanically removed or sprayed.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"text-decoration: underline; font-size: 18pt;\"\u003e\u003cstrong\u003eSolution Methodology\u003c/strong\u003e\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003eThe methods for developing a solution to this problem of plant localization and identification involves the following components:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eData sets of images of plant seedlings or crops which have not reached maturity\u003c/li\u003e\n\u003cli\u003eAn image classification model\n\u003cul\u003e\n\u003cli\u003eVerification of model\u003c/li\u003e\n\u003cli\u003eIdentify weaknesses, limitations, etc\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eObject detection model\n\u003cul\u003e\n\u003cli\u003eObject localization\u003c/li\u003e\n\u003cli\u003eIsolated image fed into classifier\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eDatasets\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eObject classification from images, when not distinctly unique, requires a large amount of training and sample data to train a model with. This was found to be particularly important for identifying and classifying unique plants.\u003c/p\u003e\n\u003cp\u003eThe following datasets were used as the primary source to train a multi-class Convolutional Neural Network (CNN) model:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://www.kaggle.com/c/plant-seedlings-classification/data\" target=\"_blank\"\u003ehttps://www.kaggle.com/c/plant-seedlings-classification/data\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.kaggle.com/vbookshelf/v2-plant-seedlings-dataset\" target=\"_blank\"\u003ehttps://www.kaggle.com/vbookshelf/v2-plant-seedlings-dataset\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.kaggle.com/tobiasnw/dandelion-image-classifier\" target=\"_blank\"\u003ehttps://www.kaggle.com/tobiasnw/dandelion-image-classifier\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAdditional datasets were investigated and examined, covering various crops, weeds, plant ages, and more, but were not included if the images did not fit reasonably in most of the following categories:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eImages containing only plants of the same type\u003c/li\u003e\n\u003cli\u003eQuality sufficiently high to determine details between species\u003c/li\u003e\n\u003cli\u003eCommon crops in seedling or early growth stages or weeds/invasive species\u003c/li\u003e\n\u003cli\u003eSufficient data to train and test on, no less the ~250-300 images\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThis focus on quality and seedlings age range for crops limited the available datasets for integration. A large majority of datasets available were of common plants, but were focused on later growth stages and larger field of views (entire fields) rather than data as would be collected on-machine with a precision agriculture method. Further, mixed plant types are not permissible for training a classifier as categories would be easily confused by the trained model and to manually sort or modify the images in usable quantities would take more time than was viable in a single semester's focus. The above linked datasets proved sufficient for determining the viability and practicality of training a CNN for classification of multiple types of plants.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eConvolutional Neural\u0026nbsp;\u003c/strong\u003e\u003cstrong\u003eNetwork\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eTo achieve the goal of multi-object classification, a Convolutional Neural Network (CNN) is seen as the best approach. CNN's are practical for image related tasks because of their input processing methodology, which breaks apart images, audio, or more into defining characteristics at several detail levels. There are three main types of layers in a CNN, convolutional layers, pooling layers, and the fully-connected (FC) layer. Keras, a subset of Tensorflow which allows for easier model creation and manipulation, was used to create and test a CNN for this project.\u003c/p\u003e\n\u003cp\u003eThe convolutional layer works by taking a multidimensional input image, namely one with a height, width, and depth (colors) and filtering pixels of the image through a feature mapping layer. The number of filters applied changes the depth of the output array, and image arrays may be filtered without iterating over every pixel, which may also be padded to ensure the entire image is processed without negating fringe pixels, such as the bottom and right\u0026nbsp; two rows \u0026amp; columns of pixels below, where the sampling method is a 3x3 matrix.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://1.cms.s81c.com/sites/default/files/2021-01-06/ICLH_Diagram_Batch_02_17A-ConvolutionalNeuralNetworks-WHITEBG.png\" alt=\"matrix multiplication in convolutional neural networks\" width=\"648\" height=\"377\"\u003e\u003c/p\u003e\n\u003cp\u003e\u0026nbsp;\u003c/p\u003e\n\u003cp\u003eThe pooling layer is used alongside the convolutional layer and down samples the input image array using either maximum value of average value pooling. This process is useful for two reasons: first it can reduce the detail to allow for new features to be determined, for example on a plant a down sample may help to indicate the width and shape of a leaf, whereas the original input would allow the notifiable features of ridges, edge details, and leaf patterns such as veins, blemishes or spots.\u003c/p\u003e\n\u003cp\u003eLastly the fully connected layer takes the outputs of the previous two layers and assigns a probable class value to the source (usually between 0 and 1), which can map to a name. This layer also connects each node to every other node in the previous layer, unlike the previous layers which tend to be partially connected instead.\u003c/p\u003e\n\u003cp\u003eFor classifying plants, a kaggle competition notebook was used as a base starting point, \u003cspan style=\"font-size: 12pt;\"\u003e\u003ca href=\"https://www.kaggle.com/limitpointinf0/crop-vs-weeds\" target=\"_blank\"\u003ehttps://www.kaggle.com/limitpointinf0/crop-vs-weeds\u003c/a\u003e. This notebook provided an image pre-processing method, simple dataset handling, and a training model. These components were valuable in creating an initial benchmark to work from on the V1 plant seedling dataset. This exact code was compared against the V2 plant seedling dataset to demonstrate the value increase in obtaining and training on more data (about 10-15% increase in image count).\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"font-size: 12pt;\"\u003eThe following points highlight the modifications made to develop and understand a classification model:\u003c/span\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cspan style=\"font-size: 12pt;\"\u003eAdd multi-dataset handling for side-by-side comparisons\u003c/span\u003e\u003c/li\u003e\n\u003cli\u003e\u003cspan style=\"font-size: 12pt;\"\u003eDevelop a predication process to validate the trained model on both previously seen and unseen images\u003c/span\u003e\u003c/li\u003e\n\u003cli\u003e\u003cspan style=\"font-size: 12pt;\"\u003eIncrease data handling process for more rapid comparison of datasets, training procedures, and more\u003c/span\u003e\u003c/li\u003e\n\u003cli\u003e\u003cspan style=\"font-size: 12pt;\"\u003eSave models and transformed image arrays\u003c/span\u003e\u003c/li\u003e\n\u003cli\u003e\u003cspan style=\"font-size: 12pt;\"\u003eDevelop visualizations and graphics of data training, verification, and prediction results\u003c/span\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cspan style=\"font-size: 12pt;\"\u003eObject detection\u003c/span\u003e\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"font-size: 12pt;\"\u003eObject detection is the process of isolating multiple objects within an single image. In this problem of precision agriculture, that includes determining plant from soil/ground from weeds to be able to feed into a classifier independently and determine the type of plant in focus. Keras provides an extensive system for object detection called RetinaNet. RetinaNet is a single stage object detector which is fast and accurate enough for many cases, including this project as the entire object detection and classification process must occur real-time on a moving machine. RetinaNet employs a feature pyramid to distinguish objects using a loss function called Focal loss, which is designed to reduce issues with foreground/background processing balances.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"font-size: 12pt;\"\u003eRetinaNet is best used as directly as an object detector feeding into a classifier in one process. This limits the usability of the previous CNN model as it is developed as a stand-alone classifier which could be ported for use in applications or other software. Furthermore, object detection is a large field with many components, which as discussed below require far more time, testing, data, and verification than was available originally in this project.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"font-size: 12pt;\"\u003eSome of the key components of object detection with RetinaNet include:\u003c/span\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cspan style=\"font-size: 12pt;\"\u003eConstructing a feature pyramid scheme\u003c/span\u003e\u003c/li\u003e\n\u003cli\u003e\u003cspan style=\"font-size: 12pt;\"\u003eBuilding the bounding boxes for objects\u003c/span\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cspan style=\"font-size: 12pt;\"\u003ecenter point bounding box\u003c/span\u003e\u003c/li\u003e\n\u003cli\u003e\u003cspan style=\"font-size: 12pt;\"\u003ecorner-defined bounding box\u003c/span\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cspan style=\"font-size: 12pt;\"\u003eIntegrating a classifier\u003c/span\u003e\u003c/li\u003e\n\u003cli\u003e\u003cspan style=\"font-size: 12pt;\"\u003eBuilding a detection handler for feeding data into the RetinaNet model\u003c/span\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u0026nbsp;\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"text-decoration: underline;\"\u003e\u003cstrong\u003e\u003cspan style=\"font-size: 18pt;\"\u003eResults\u003c/span\u003e\u003c/strong\u003e\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"font-size: 12pt;\"\u003eDuring and after implementation of the classification model components, several visualizations and data insights were created.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"font-size: 12pt;\"\u003eThe following charts show training and validation accuracy and loss values over 25 epochs on both versions of the seedling datasets, on a batch size of 10.\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://usu.instructure.com/users/1298792/files/81034319/preview?verifier=Nll4wyEyLiER7xQjNJ6cCzj3QiEPRkJ9dX1ODoOR\" alt=\"20_epoch_train_v1.png\" width=\"650\" height=\"434\" data-api-endpoint=\"https://usu.instructure.com/api/v1/users/1298792/files/81034319\" data-api-returntype=\"File\"\u003e \u0026nbsp;\u003cimg src=\"https://usu.instructure.com/users/1298792/files/81034329/preview?verifier=7s2vFgOE9wMAY3azdug8VLYmFRGRcuEW7hEVBwRq\" alt=\"20_epoch_train_v2.png\" width=\"651\" height=\"433\" data-api-endpoint=\"https://usu.instructure.com/api/v1/users/1298792/files/81034329\" data-api-returntype=\"File\"\u003e\u0026nbsp;\u0026nbsp;\u003c/p\u003e\n\u003cp\u003eBy examining the loss values on each, we can realize overfitting levels and determine the approximate best number of epochs to train a final model. Note that in these two training models, the V2 validation loss, despite increasing over the remaining 20 epochs, remains lower than the V1 validation loss. This is attributed to the approximate 10% increase in images available in the V2 dataset, which allows for slightly longer training without as much risk of memorizing the specific features of the images available.\u003c/p\u003e\n\u003cp\u003eUsing this information a final model of each dataset version was trained to 25 epochs with a batch size 20. This batch size was chosen due to the increase in images available by incorporating the complete dataset into the training, allowing more training weight updates in general, while also slightly reducing the number of weight modifications to prevent overfitting on this specific data. This final model for the V2 dataset training appeared as follows:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://usu.instructure.com/users/1298792/files/81034406/preview?verifier=uszT4QvSV2Ti8UyXrL5vRR118lUik7IGpOFB97nk\" alt=\"final-training-model_v2.jpg\" width=\"712\" height=\"356\" data-api-endpoint=\"https://usu.instructure.com/api/v1/users/1298792/files/81034406\" data-api-returntype=\"File\"\u003e\u0026nbsp;\u0026nbsp;\u003c/p\u003e\n\u003cp\u003eAs can be seen, this model trains to a predicted accuracy of 99+% and a near 0 loss. While this looks fantastic, it requires an examination of actual predictions to indicate if there is any true level of accuracy. This was done using hand selected images sampled from both the V2 dataset trained on and various images found on the internet. The results of the predictions are indicated in the following heatmap as a confusion matrix\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://usu.instructure.com/users/1298792/files/81034503/preview?verifier=F9JbP6LjSakfI8mK7nLel3XI12dBDPRMkjZ9j91p\" alt=\"prediction_heatmap_v2_model.png\" width=\"587\" height=\"484\" data-api-endpoint=\"https://usu.instructure.com/api/v1/users/1298792/files/81034503\" data-api-returntype=\"File\"\u003e \u0026nbsp;\u0026nbsp;\u0026nbsp;\u003c/p\u003e\n\u003cp\u003eAs seen in this heatmap, several categories are confused in this model. Many of the \"confused\" classifications were predictions on images sourced from the internet, not trained on and not seen before. Common Wheat is confused most often as Black grass. Further, Black grass is also confused as Maize (corn) in a surprisingly high frequency, alongside several other confusions. This latter part is surprising as maize seedlings tend to look unique (see annotated images below). Here one can examine why Wheat, Black grass and Silky bent (pictured in said order) may often be confused for each other, even to a human observer:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://usu.instructure.com/users/1298792/files/81034565/preview?verifier=YHraOQl6krM8PpVHBEUsJdSjEFAv7VlXlnDrJcFw\" alt=\"wheat.png\" width=\"300\" height=\"300\" data-api-endpoint=\"https://usu.instructure.com/api/v1/users/1298792/files/81034565\" data-api-returntype=\"File\"\u003e \u0026nbsp;\u003cimg src=\"https://usu.instructure.com/users/1298792/files/81034572/preview?verifier=guPh3RB8Y2WYEaVrKxUvImZagXd9y5K4n0VLVg7J\" alt=\"black-grass.png\" width=\"283\" height=\"300\" data-api-endpoint=\"https://usu.instructure.com/api/v1/users/1298792/files/81034572\" data-api-returntype=\"File\"\u003e \u0026nbsp;\u003cimg src=\"https://usu.instructure.com/users/1298792/files/81034576/preview?verifier=u3mZdNfP0pB3MHMHwKiloISvLXboK9TXy3ob06Tu\" alt=\"loose silky bent.png\" width=\"300\" height=\"300\" data-api-endpoint=\"https://usu.instructure.com/api/v1/users/1298792/files/81034576\" data-api-returntype=\"File\"\u003e\u0026nbsp;\u0026nbsp;\u003c/p\u003e\n\u003cp\u003e\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; Wheat\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp;Black Grass\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp;Loose Silky Bent\u003c/p\u003e\n\u003cp\u003e\u0026nbsp;\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://usu.instructure.com/users/1298792/files/81034622/preview?verifier=1weoQ7r5PKyYUFdLFuWdqyqmzyPn90V7WvxsUWpO\" alt=\"maize.png\" width=\"269\" height=\"269\" data-api-endpoint=\"https://usu.instructure.com/api/v1/users/1298792/files/81034622\" data-api-returntype=\"File\"\u003e\u0026nbsp;\u0026nbsp;\u003c/p\u003e\n\u003cp\u003e\u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp;Maize (Corn)\u003c/p\u003e\n\u003cp\u003e\u0026nbsp;\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"text-decoration: underline;\"\u003e\u003cstrong\u003e\u003cspan style=\"font-size: 18pt;\"\u003eConclusion \u0026amp; Summary\u003c/span\u003e\u003c/strong\u003e\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eClassification\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003ePlant images present a number of unique challenges in producing a valuable and accurate classifier. These challenges include:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDataset training size\u003c/li\u003e\n\u003cli\u003eRegion/application specialization\u003c/li\u003e\n\u003cli\u003ePlant similarities\u003c/li\u003e\n\u003cli\u003eRobust predictions from lower-quality images\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cspan style=\"text-decoration: underline;\"\u003eTraining size\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003eA valuable and useful model would require a training library several times larger than that present in this project. This is due to the issue of overfitting in a relatively low number of epochs. To produce a more generic plant classifier, more data would be required than in a specialized classifier, and more data will require more storage handling, greater training time, and more efficient data handling. All together, it is viable to create a general plant classifier, even to more classes than was demonstrated here, with the caveat that potentially magnitudes larger training data sets (in terms of number of images) would be required\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"text-decoration: underline;\"\u003eSpecialization\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003eAs mentioned, specialization would potentially permit a smaller training set to generate a practical and accurate classifier in a short period of time. Specialization could include manually determining specific plants to focus on for a field type (say corn vs wheat fields) and local weeds or pests. The classifier could be trained to handle the specific types, which decreases the risk of confusion. These specialized classifiers could easily be programatically switched for handling differences between growing season, year-to-year crop cycling, and other equipment reuse as specified by the user or farmer.\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"text-decoration: underline;\"\u003eSimilarities\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003eMany plants are similar in nature to one another as far as defining structures are concerned. For example, as seen above wheat and grasses or grass-like plants are quite similar in most cases. Only one of those would ever be desired as a crop, Wheat, which was falsely predicted as Black-grass 100% of the time in the small test performed. This would be unacceptable in a production or end user environment, but could be rectified by the use of more training data to differentiate the similarities, specialization of classifiers (if a similar plant type will never or rarely be present, don't try to handle classifying it), or training with more diverse data such as differently lighted images, more varied growth stages, and/or higher detailed images.\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"text-decoration: underline;\"\u003eRobust predictions\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003eThe likely application of this classifier is with a live-fed stream of data from a tractor mounted camera system as would be found in a Precision Agriculture setup. These images may not be well focused, or with variable lighting between images and plant quantities or clarity present for various reasons. A classification model could be built, guarded against overfitting and with sufficient, varied training data, to handle lower-quality images and still provide a reasonable prediction accuracy.\u003c/p\u003e\n\u003cp\u003e\u0026nbsp;\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eThus in conclusion, \u003c/strong\u003eit is practical and possible to create a reliable plant classifier for use in precision agriculture applications. To do so effectively one must take into account the issues of training data available, specialization requirements per application, knowledge of similarly-shaped plant species which may be present, and the methods by which the data will be collected. If these factors are considered carefully when determining a model to train, it is very reasonable a highly accurate plant classifier could be produced and implemented in production-grade agriculture systems.\u003c/p\u003e\n\u003cp\u003e\u0026nbsp;\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"text-decoration: underline; font-size: 18pt;\"\u003e\u003cstrong\u003eFuture\u0026nbsp;\u003c/strong\u003e\u003c/span\u003e\u003cspan style=\"font-size: 24px;\"\u003e\u003cstrong\u003e\u003cu\u003eProgress\u003c/u\u003e\u003c/strong\u003e\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003eFuture work would be to establish the following items for true usability in a Precision Ag applications:\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eObject Detection\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eTo identify where and define all the plants in an image as fed to the software by a camera mount, an object detector model would need to be used. This model requires a higher rate of speed over near-perfect accuracy, and as such could make use of the Keras RetinaNet single stage detector. This would permit real-time use on machinery. The object detection process would also provide the relative location of plants which could be mapped to a mechanical armature movement allowing for the actual mechanical weeding or spot spraying to occur.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eNew Class Integration\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eTo best make use of specialization practices, some interface or efficient methodology for providing data with appropriately labelled classes should be established. New crop types, weeds (such as dandelions) and more should be easily integrated into a new model for training and classifier producing.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eNotes on images\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eImage sources for classification training should contain a single to no more than 3 of the same plant in the image. A common issue present in many datasets was the absence of isolated plants, which would easily falsely train a classifier to recognize some plants as others. This was most common in the dandelion sources found. Dandelions are a very common weed and would be easily recognizable, but the primary large datasets available were comprised mainly of dandelions embedded in grasses or leaves, which is strikingly similar to many of the narrow-leafed plant types already trained as shown in this project. Note that in an average cropland, some conditions like thick grass everywhere are extremely rare or nonexistent situations. Below is an example of the average dandelion image from the best currently available datasets.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://usu.instructure.com/users/1298792/files/81053896/preview?verifier=8VvZboAMBLqmA0w95id4rDP8PVIVk0x1przokqAI\" alt=\"IMG_1149.jpg\" width=\"403\" height=\"403\" data-api-endpoint=\"https://usu.instructure.com/api/v1/users/1298792/files/81053896\" data-api-returntype=\"File\"\u003e\u0026nbsp;\u0026nbsp;\u003c/p\u003e\n\u003cp\u003e\u0026nbsp;\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"text-decoration: underline; font-size: 18pt;\"\u003e\u003cstrong\u003eOther Components\u003c/strong\u003e\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003eResearch:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://www.nature.com/articles/s41598-018-38343-3\" target=\"_blank\"\u003ehttps://www.nature.com/articles/s41598-018-38343-3\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7305380/\" target=\"_blank\"\u003ehttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC7305380/\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.sciencedirect.com/science/article/pii/S0168169920312709\" target=\"_blank\"\u003ehttps://www.sciencedirect.com/science/article/pii/S0168169920312709\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.researchgate.net/publication/326211032_Machine_Vision_Retrofit_System_for_Mechanical_Weed_Control_in_Precision_Agriculture_Applications\" target=\"_blank\"\u003ehttps://www.researchgate.net/publication/326211032_Machine_Vision_Retrofit_System_for_Mechanical_Weed_Control_in_Precision_Agriculture_Applications\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cspan style=\"text-decoration: underline;\"\u003e\u003cspan style=\"font-size: 14pt;\"\u003eTutorials/Training:\u003c/span\u003e\u003c/span\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://www.simplilearn.com/tutorials/deep-learning-tutorial/neural-network\" target=\"_blank\"\u003ehttps://www.simplilearn.com/tutorials/deep-learning-tutorial/neural-network\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://learnopencv.com/image-classification-using-convolutional-neural-networks-in-keras/\" target=\"_blank\"\u003ehttps://learnopencv.com/image-classification-using-convolutional-neural-networks-in-keras/\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://stackabuse.com/image-recognition-in-python-with-tensorflow-and-keras/\" target=\"_blank\"\u003ehttps://stackabuse.com/image-recognition-in-python-with-tensorflow-and-keras/\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.codespeedy.com/how-to-choose-number-of-epochs-to-train-a-neural-network-in-keras/\" target=\"_blank\"\u003ehttps://www.codespeedy.com/how-to-choose-number-of-epochs-to-train-a-neural-network-in-keras/\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u0026nbsp;\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"font-size: 18pt;\"\u003e\u003cstrong\u003ePresentations\u003c/strong\u003e\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"text-decoration: underline;\"\u003e\u003cspan style=\"font-size: 12pt;\"\u003eFinal Presentation and Results\u003c/span\u003e\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003ciframe title=\"embedded content\" src=\"https://docs.google.com/presentation/d/e/2PACX-1vQFHy3zl2bM4MnbAYMLlc_BEO4o7gbAkHfmqWQIkTuxoRsF3fMbKO-c-qPHF9Y4xZQraUTIMnSML0sI/embed?start=false\u0026amp;loop=true\u0026amp;delayms=15000\" width=\"960\" height=\"569\" allowfullscreen=\"allowfullscreen\" webkitallowfullscreen=\"webkitallowfullscreen\" mozallowfullscreen=\"mozallowfullscreen\"\u003e\u003c/iframe\u003e\u0026nbsp;\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"text-decoration: underline;\"\u003e\u003cspan style=\"font-size: 12pt;\"\u003eProject Midpoint Update\u003c/span\u003e\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003ciframe title=\"embedded content\" src=\"https://docs.google.com/presentation/d/e/2PACX-1vTOy4nyE98T-4VkFLYVNn4mNevVODc8L5ekgaAfzmTnM-PBKtdiKIiKWzdguKl_1dHjLY_fWh_0NNLS/embed?start=false\u0026amp;loop=false\u0026amp;delayms=5000\" width=\"960\" height=\"569\" allowfullscreen=\"allowfullscreen\" webkitallowfullscreen=\"webkitallowfullscreen\" mozallowfullscreen=\"mozallowfullscreen\"\u003e\u003c/iframe\u003e\u0026nbsp;\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"text-decoration: underline;\"\u003e\u003cspan style=\"font-size: 12pt;\"\u003eProject Overview\u003c/span\u003e\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca class=\"instructure_file_link instructure_scribd_file\" title=\"Project In Short-2.pptx\" href=\"https://usu.instructure.com/users/1298792/files/80452806?wrap=1\u0026amp;verifier=hFxK2kcHs6xBpxw4nZlDufkh9FoQ7aWwONcQSrxf\" target=\"_blank\" data-canvas-previewable=\"false\" data-api-endpoint=\"https://usu.instructure.com/api/v1/users/1298792/files/80452806\" data-api-returntype=\"File\"\u003eProject In Short.pptx\u003c/a\u003e\u0026nbsp;\u0026nbsp;\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"text-decoration: underline;\"\u003e\u003cspan style=\"font-size: 12pt;\"\u003eConcept Background\u003c/span\u003e\u003c/span\u003e\u003c/p\u003e\n\u003cp\u003e\u0026nbsp; \u003ca class=\"instructure_file_link instructure_scribd_file\" title=\"Presentation_Precision_AG.pptx\" href=\"https://usu.instructure.com/users/1298792/files/80084081?wrap=1\u0026amp;verifier=UDi3MmOijXQhLcSZByaXlUFCbRUruyqDB8yVlI3E\" target=\"_blank\" data-canvas-previewable=\"false\" data-api-endpoint=\"https://usu.instructure.com/api/v1/users/1298792/files/80084081\" data-api-returntype=\"File\"\u003ePresentation_Precision_AG.pptx\u003c/a\u003e\u0026nbsp;\u0026nbsp;\u003c/p\u003e\n\u003cp\u003e\u0026nbsp;\u003c/p\u003e\n\u003cp\u003e\u003cspan style=\"font-size: 18pt;\"\u003e\u003cstrong\u003eAbout Me\u003c/strong\u003e\u003c/span\u003e\u003c/p\u003e\n\u003ctable style=\"border-collapse: collapse; width: 98%; height: 294px; border-style: hidden;\" border=\"0\" cellpadding=\"5\"\u003e\n\u003ctbody\u003e\n\u003ctr style=\"height: 294px;\"\u003e\n\u003ctd style=\"width: 49.9072%; height: 294px;\"\u003e\n\u003cp\u003eCurrently a senior pursuing a CS Bachelorâ€™s degree\u003c/p\u003e\n\u003cp\u003eMachine learning experience:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAdvanced Algorithms, limited supplemental reading\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eExperience/Skills:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003egit (Github, bitbucket, etc), Jira, remote work tools (slack, teams, discord, zoom, etc)\u003c/li\u003e\n\u003cli\u003ePython, Java, javascript, some web/full-stack, SQL and NoSQL DB\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eInterests and History:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eWorked 6 years in Agriculture research/GMO products for biofuels, food products, and livestock feed\u003c/li\u003e\n\u003cli\u003eOriginally from IA, ~40-45% of electricity production is wind\u003c/li\u003e\n\u003cli\u003eBasic familiarity with home geothermal system for HVAC\u003c/li\u003e\n\u003cli\u003eInterest in smaller-scale power systems (local and home power grids/systems)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/td\u003e\n\u003ctd style=\"width: 50%; height: 294px;\"\u003e\u003cimg src=\"https://usu.instructure.com/users/1298792/files/80083943/preview?verifier=j9w7aTW9gilFHQzFbMOrUIz71JaXDvva0zBPxeMn\" alt=\"piano_portrait.jpg\" width=\"217\" height=\"290\" data-api-endpoint=\"https://usu.instructure.com/api/v1/users/1298792/files/80083943\" data-api-returntype=\"File\"\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e\u0026nbsp;\u003c/p\u003e\n\u003ch4\u003eDiscussion Board for Comments:\u003c/h4\u003e\n\u003cp\u003e\u003ca title=\"Jake Cook - Plant ID with CNN's\" href=\"https://usu.instructure.com/courses/639034/discussion_topics/2264163\" data-api-endpoint=\"https://usu.instructure.com/api/v1/courses/639034/discussion_topics/2264163\" data-api-returntype=\"Discussion\"\u003eJake Cook - Plant ID with CNN's\u003c/a\u003e\u0026nbsp;\u003c/p\u003e" }, "WIKI_PAGE_REVISION": "25", "WIKI_PAGE_SHOW_PATH": "/courses/639034/pages/jake-cooks-portfolio", "WIKI_PAGE_EDIT_PATH": "/courses/639034/pages/jake-cooks-portfolio/edit", "WIKI_PAGE_HISTORY_PATH": "/courses/639034/pages/jake-cooks-portfolio/revisions", "COURSE_ID": "639034", "MODULES_PATH": "/courses/639034/modules", "wiki_page_menu_tools": [], "wiki_index_menu_tools": [], "DISPLAY_SHOW_ALL_LINK": true, "CAN_SET_TODO_DATE": false, "IMMERSIVE_READER_ENABLED": false, "badge_counts": { "submissions": 0 }, "notices": [], "active_context_tab": "pages" };
  </script>

  <link rel="preload"
    href="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/variables-8391c84da435c9cfceea2b2b3317ff66.js"
    as="script" type="text/javascript">
  <link rel="preload" href="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/Denver-40670c6af7.js"
    as="script" type="text/javascript">
  <link rel="preload" href="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/Denver-40670c6af7.js"
    as="script" type="text/javascript">
  <link rel="preload" href="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/en_US-80a0ce259b.js"
    as="script" type="text/javascript">
  <link rel="preload" href="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/main-e-a60cb0321e.js"
    as="script" type="text/javascript">
  <script>
    //<![CDATA[

    ;["https://du11hjcvx0uqb.cloudfront.net/br/dist/brandable_css/61451b4c24553b78ff8eb71dd6ee3b6a/variables-8391c84da435c9cfceea2b2b3317ff66.js", "https://du11hjcvx0uqb.cloudfront.net/br/dist/timezone/America/Denver-40670c6af7.js", "https://du11hjcvx0uqb.cloudfront.net/br/dist/timezone/America/Denver-40670c6af7.js", "https://du11hjcvx0uqb.cloudfront.net/br/dist/timezone/en_US-80a0ce259b.js", "https://du11hjcvx0uqb.cloudfront.net/br/dist/webpack-production/main-e-a60cb0321e.js"].forEach(function (src) {
      var s = document.createElement('script')
      s.src = src
      s.async = false
      document.head.appendChild(s)
    });
//]]>
  </script>
  <script
    src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/variables-8391c84da435c9cfceea2b2b3317ff66.js"></script>
  <script src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/Denver-40670c6af7.js"></script>
  <script src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/Denver-40670c6af7.js"></script>
  <script src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/en_US-80a0ce259b.js"></script>
  <script src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/main-e-a60cb0321e.js"></script>
  <link rel="preload" href="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/0-c-aa7dbd72bb.js" as="script"
    type="text/javascript">
  <link rel="preload" href="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/1-c-c62473714a.js" as="script"
    type="text/javascript">
  <link rel="preload" href="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/2-c-49d1ef7c2e.js" as="script"
    type="text/javascript">
  <link rel="preload" href="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/3-c-e5559a96fe.js" as="script"
    type="text/javascript">
  <link rel="preload" href="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/4-c-108302bd1a.js" as="script"
    type="text/javascript">
  <link rel="preload" href="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/5-c-2e0137336a.js" as="script"
    type="text/javascript">
  <link rel="preload" href="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/6-c-b5dbcf1cf8.js" as="script"
    type="text/javascript">
  <link rel="preload" href="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/7-c-1ee6c428d1.js" as="script"
    type="text/javascript">
  <link rel="preload" href="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/8-c-b3bee56cf0.js" as="script"
    type="text/javascript">
  <link rel="preload" href="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/9-c-b78c239529.js" as="script"
    type="text/javascript">
  <link rel="preload" href="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/10-c-c9d6a7e1b2.js"
    as="script" type="text/javascript">
  <link rel="preload" href="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/11-c-f6c4a1c3ed.js"
    as="script" type="text/javascript">
  <link rel="preload" href="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/15-c-222fff536f.js"
    as="script" type="text/javascript">
  <link rel="preload" href="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/19-c-a8b0a13314.js"
    as="script" type="text/javascript">
  <link rel="preload" href="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/24-c-37d14b4269.js"
    as="script" type="text/javascript">
  <link rel="preload" href="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/25-c-343739d579.js"
    as="script" type="text/javascript">
  <link rel="preload" href="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/27-c-34aab7e7ba.js"
    as="script" type="text/javascript">
  <link rel="preload" href="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/30-c-d47e714283.js"
    as="script" type="text/javascript">
  <link rel="preload" href="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/32-c-5a482fa1e6.js"
    as="script" type="text/javascript">
  <link rel="preload" href="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/34-c-798a700150.js"
    as="script" type="text/javascript">
  <link rel="preload" href="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/35-c-442fb022b9.js"
    as="script" type="text/javascript">
  <link rel="preload" href="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/38-c-d03705fb6c.js"
    as="script" type="text/javascript">
  <link rel="preload" href="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/40-c-8022dfc6cd.js"
    as="script" type="text/javascript">
  <link rel="preload" href="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/41-c-c1c12477ca.js"
    as="script" type="text/javascript">
  <link rel="preload" href="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/46-c-dc388f3363.js"
    as="script" type="text/javascript">
  <link rel="preload" href="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/43-c-f2699654d3.js"
    as="script" type="text/javascript">
  <link rel="preload" href="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/50-c-bebf9aa8b5.js"
    as="script" type="text/javascript">
  <link rel="preload" href="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/56-c-3cfcc5fc1d.js"
    as="script" type="text/javascript">
  <link rel="preload" href="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/58-c-1b6e27b54e.js"
    as="script" type="text/javascript">
  <link rel="preload"
    href="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/wiki_page_show-c-cbba4cc2e9.js" as="script"
    type="text/javascript">
  <script>
    //<![CDATA[
    (window.bundles || (window.bundles = [])).push('wiki_page_show');
    (window.bundles || (window.bundles = [])).push('navigation_header');
//]]>
  </script>
  <title>Jake Cook's Portfolio: Spring 2021 CS-6620-LW1 XL</title>

  <script>
    window.ga = window.ga || function () { (ga.q = ga.q || []).push(arguments) }; ga.l = +new Date;
    ga('create', "UA-9138420-1", 'auto');
    ga('set', 'userId', "e90c0042951f44674fd6bb0fa76bb53b");
    ga('set', 'dimension1', "100");
    ga('set', 'dimension2', "00");
    ga('set', 'dimension3', "0");
    ga('set', 'dimension4', "Higher Ed");
    ga('send', 'pageview');
    (window.requestIdleCallback || window.setTimeout)(function () {
      var s = document.createElement('script'), m = document.getElementsByTagName('script')[0]
      s.async = 1; s.src = 'https://www.google-analytics.com/analytics.js'
      m.parentNode.insertBefore(s, m)
    });
  </script>



  <script src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/canvas_global_redirect.js"></script>
  <style type="text/css"></style>
  <script charset="utf-8"
    src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/0-c-aa7dbd72bb.js"></script>
  <script charset="utf-8"
    src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/1-c-c62473714a.js"></script>
  <script charset="utf-8"
    src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/2-c-49d1ef7c2e.js"></script>
  <script charset="utf-8"
    src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/3-c-e5559a96fe.js"></script>
  <script charset="utf-8"
    src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/4-c-108302bd1a.js"></script>
  <script charset="utf-8"
    src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/5-c-2e0137336a.js"></script>
  <script charset="utf-8"
    src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/6-c-b5dbcf1cf8.js"></script>
  <script charset="utf-8"
    src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/7-c-1ee6c428d1.js"></script>
  <script charset="utf-8"
    src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/8-c-b3bee56cf0.js"></script>
  <script charset="utf-8"
    src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/9-c-b78c239529.js"></script>
  <script charset="utf-8"
    src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/10-c-c9d6a7e1b2.js"></script>
  <script charset="utf-8"
    src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/11-c-f6c4a1c3ed.js"></script>
  <script charset="utf-8"
    src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/15-c-222fff536f.js"></script>
  <script charset="utf-8"
    src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/19-c-a8b0a13314.js"></script>
  <script charset="utf-8"
    src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/24-c-37d14b4269.js"></script>
  <script charset="utf-8"
    src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/25-c-343739d579.js"></script>
  <script charset="utf-8"
    src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/27-c-34aab7e7ba.js"></script>
  <script charset="utf-8"
    src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/30-c-d47e714283.js"></script>
  <script charset="utf-8"
    src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/32-c-5a482fa1e6.js"></script>
  <script charset="utf-8"
    src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/34-c-798a700150.js"></script>
  <script charset="utf-8"
    src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/35-c-442fb022b9.js"></script>
  <script charset="utf-8"
    src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/38-c-d03705fb6c.js"></script>
  <script charset="utf-8"
    src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/40-c-8022dfc6cd.js"></script>
  <script charset="utf-8"
    src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/41-c-c1c12477ca.js"></script>
  <script charset="utf-8"
    src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/46-c-dc388f3363.js"></script>
  <script charset="utf-8"
    src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/43-c-f2699654d3.js"></script>
  <script charset="utf-8"
    src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/50-c-bebf9aa8b5.js"></script>
  <script charset="utf-8"
    src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/56-c-3cfcc5fc1d.js"></script>
  <script charset="utf-8"
    src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/58-c-1b6e27b54e.js"></script>
  <script charset="utf-8"
    src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/wiki_page_show-c-cbba4cc2e9.js"></script>
  <script charset="utf-8"
    src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/inst_fs_service_worker-c-a44e3dc046.js"></script>
  <script charset="utf-8"
    src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/690-c-a2f4783386.js"></script>
  <style type="text/css" data-glamor=""></style>
  <script charset="utf-8"
    src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/18-c-8cb7eb08a0.js"></script>
  <script charset="utf-8"
    src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/22-c-0b9170e42e.js"></script>
  <script charset="utf-8"
    src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/23-c-0ad2055f79.js"></script>
  <script charset="utf-8"
    src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/31-c-e70690db80.js"></script>
  <script charset="utf-8"
    src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/42-c-f94f72e505.js"></script>
  <script charset="utf-8"
    src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/76-c-f075640429.js"></script>
  <script charset="utf-8"
    src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/102-c-ed0f9171d1.js"></script>
  <script charset="utf-8"
    src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/689-c-613aa6d58f.js"></script>
  <style type="text/css">
    .mejs-offscreen {
      clip: rect(1px 1px 1px 1px);
      clip: rect(1px, 1px, 1px, 1px);
      clip-path: polygon(0px 0, 0 0, 0 0, 0 0);
      position: absolute !important;
      height: 1px;
      width: 1px;
      overflow: hidden
    }

    .mejs-container {
      position: relative;
      background: #000;
      font-family: Helvetica, Arial, serif;
      text-align: left;
      vertical-align: top;
      text-indent: 0
    }

    .mejs-fill-container,
    .mejs-fill-container .mejs-container {
      width: 100%;
      height: 100%
    }

    .mejs-fill-container {
      overflow: hidden
    }

    .mejs-container:focus {
      outline: 0
    }

    .me-plugin {
      position: absolute
    }

    .mejs-embed,
    .mejs-embed body {
      width: 100%;
      height: 100%;
      margin: 0;
      padding: 0;
      background: #000;
      overflow: hidden
    }

    .mejs-fullscreen {
      overflow: hidden !important
    }

    .mejs-container-fullscreen {
      position: fixed;
      left: 0;
      top: 0;
      right: 0;
      bottom: 0;
      overflow: hidden;
      z-index: 1000
    }

    .mejs-container-fullscreen .mejs-mediaelement,
    .mejs-container-fullscreen video {
      width: 100%;
      height: 100%
    }

    .mejs-clear {
      clear: both
    }

    .mejs-background {
      position: absolute;
      top: 0;
      left: 0
    }

    .mejs-mediaelement {
      position: absolute;
      top: 0;
      left: 0;
      width: 100%;
      height: 100%
    }

    .mejs-poster {
      position: absolute;
      top: 0;
      left: 0;
      background-size: contain;
      background-position: 50% 50%;
      background-repeat: no-repeat
    }

    :root .mejs-poster img {
      display: none
    }

    .mejs-poster img {
      border: 0;
      padding: 0
    }

    .mejs-overlay {
      position: absolute;
      top: 0;
      left: 0
    }

    .mejs-overlay-play {
      cursor: pointer
    }

    .mejs-overlay-button {
      position: absolute;
      top: 50%;
      left: 50%;
      width: 100px;
      height: 100px;
      margin: -50px 0 0 -50px;
      background: url(https://du11hjcvx0uqb.cloudfront.net/br/dist/webpack-production/746c3af7a145a09239a36e5ef61cfea0.svg) no-repeat
    }

    .no-svg .mejs-overlay-button {
      background-image: url(https://du11hjcvx0uqb.cloudfront.net/br/dist/webpack-production/716436fb3df0d29e6b37dd62d952676a.png)
    }

    .mejs-overlay:hover .mejs-overlay-button {
      background-position: 0 -100px
    }

    .mejs-overlay-loading {
      position: absolute;
      top: 50%;
      left: 50%;
      width: 80px;
      height: 80px;
      margin: -40px 0 0 -40px;
      background: #333;
      background: url(https://du11hjcvx0uqb.cloudfront.net/br/dist/webpack-production/703c659e4bf563a05c6338a1727e006c.png);
      background: rgba(0, 0, 0, .9);
      background: -webkit-gradient(linear, 0 0, 0 100%, from(rgba(50, 50, 50, .9)), to(rgba(0, 0, 0, .9)));
      background: -webkit-linear-gradient(top, rgba(50, 50, 50, .9), rgba(0, 0, 0, .9));
      background: -moz-linear-gradient(top, rgba(50, 50, 50, .9), rgba(0, 0, 0, .9));
      background: -o-linear-gradient(top, rgba(50, 50, 50, .9), rgba(0, 0, 0, .9));
      background: -ms-linear-gradient(top, rgba(50, 50, 50, .9), rgba(0, 0, 0, .9));
      background: linear-gradient(rgba(50, 50, 50, .9), rgba(0, 0, 0, .9))
    }

    .mejs-overlay-loading span {
      display: block;
      width: 80px;
      height: 80px;
      background: transparent url(https://du11hjcvx0uqb.cloudfront.net/br/dist/webpack-production/76b326f4d44222126fee21076595bef5.gif) 50% 50% no-repeat
    }

    .mejs-container .mejs-controls {
      position: absolute;
      list-style-type: none;
      margin: 0;
      padding: 0;
      bottom: 0;
      left: 0;
      background: url(https://du11hjcvx0uqb.cloudfront.net/br/dist/webpack-production/703c659e4bf563a05c6338a1727e006c.png);
      background: rgba(0, 0, 0, .7);
      background: -webkit-gradient(linear, 0 0, 0 100%, from(rgba(50, 50, 50, .7)), to(rgba(0, 0, 0, .7)));
      background: -webkit-linear-gradient(top, rgba(50, 50, 50, .7), rgba(0, 0, 0, .7));
      background: -moz-linear-gradient(top, rgba(50, 50, 50, .7), rgba(0, 0, 0, .7));
      background: -o-linear-gradient(top, rgba(50, 50, 50, .7), rgba(0, 0, 0, .7));
      background: -ms-linear-gradient(top, rgba(50, 50, 50, .7), rgba(0, 0, 0, .7));
      background: linear-gradient(rgba(50, 50, 50, .7), rgba(0, 0, 0, .7));
      height: 30px;
      width: 100%
    }

    .mejs-container .mejs-controls div {
      list-style-type: none;
      background-image: none;
      display: block;
      float: left;
      margin: 0;
      padding: 0;
      width: 26px;
      height: 26px;
      font-size: 11px;
      line-height: 11px;
      font-family: Helvetica, Arial, serif;
      border: 0
    }

    .mejs-controls .mejs-button button {
      cursor: pointer;
      display: block;
      font-size: 0;
      line-height: 0;
      text-decoration: none;
      margin: 7px 5px;
      padding: 0;
      position: absolute;
      height: 16px;
      width: 16px;
      border: 0;
      background: transparent url(https://du11hjcvx0uqb.cloudfront.net/br/dist/webpack-production/40f56f5a736da4effeb790cedb8a52f0.svg) no-repeat
    }

    .no-svg .mejs-controls .mejs-button button {
      background-image: url(https://du11hjcvx0uqb.cloudfront.net/br/dist/webpack-production/24a0227fbdd3acfd86ff03fc3fc6c8a4.png)
    }

    .mejs-controls .mejs-button button:focus {
      outline: dotted 1px #999
    }

    .mejs-container .mejs-controls .mejs-time {
      color: #fff;
      display: block;
      height: 17px;
      width: auto;
      padding: 10px 3px 0;
      overflow: hidden;
      text-align: center;
      -moz-box-sizing: content-box;
      -webkit-box-sizing: content-box;
      box-sizing: content-box
    }

    .mejs-container .mejs-controls .mejs-time a {
      color: #fff;
      font-size: 11px;
      line-height: 12px;
      display: block;
      float: left;
      margin: 1px 2px 0 0;
      width: auto
    }

    .mejs-controls .mejs-play button {
      background-position: 0 0
    }

    .mejs-controls .mejs-pause button {
      background-position: 0 -16px
    }

    .mejs-controls .mejs-stop button {
      background-position: -112px 0
    }

    .mejs-controls div.mejs-time-rail {
      direction: ltr;
      width: 200px;
      padding-top: 5px
    }

    .mejs-controls .mejs-time-rail span,
    .mejs-controls .mejs-time-rail a {
      display: block;
      position: absolute;
      width: 180px;
      height: 10px;
      -webkit-border-radius: 2px;
      -moz-border-radius: 2px;
      border-radius: 2px;
      cursor: pointer
    }

    .mejs-controls .mejs-time-rail .mejs-time-total {
      margin: 5px;
      background: #333;
      background: rgba(50, 50, 50, .8);
      background: -webkit-gradient(linear, 0 0, 0 100%, from(rgba(30, 30, 30, .8)), to(rgba(60, 60, 60, .8)));
      background: -webkit-linear-gradient(top, rgba(30, 30, 30, .8), rgba(60, 60, 60, .8));
      background: -moz-linear-gradient(top, rgba(30, 30, 30, .8), rgba(60, 60, 60, .8));
      background: -o-linear-gradient(top, rgba(30, 30, 30, .8), rgba(60, 60, 60, .8));
      background: -ms-linear-gradient(top, rgba(30, 30, 30, .8), rgba(60, 60, 60, .8));
      background: linear-gradient(rgba(30, 30, 30, .8), rgba(60, 60, 60, .8))
    }

    .mejs-controls .mejs-time-rail .mejs-time-buffering {
      width: 100%;
      background-image: -o-linear-gradient(-45deg, rgba(255, 255, 255, .15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, .15) 50%, rgba(255, 255, 255, .15) 75%, transparent 75%, transparent);
      background-image: -webkit-gradient(linear, 0 100%, 100% 0, color-stop(0.25, rgba(255, 255, 255, .15)), color-stop(0.25, transparent), color-stop(0.5, transparent), color-stop(0.5, rgba(255, 255, 255, .15)), color-stop(0.75, rgba(255, 255, 255, .15)), color-stop(0.75, transparent), to(transparent));
      background-image: -webkit-linear-gradient(-45deg, rgba(255, 255, 255, .15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, .15) 50%, rgba(255, 255, 255, .15) 75%, transparent 75%, transparent);
      background-image: -moz-linear-gradient(-45deg, rgba(255, 255, 255, .15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, .15) 50%, rgba(255, 255, 255, .15) 75%, transparent 75%, transparent);
      background-image: -ms-linear-gradient(-45deg, rgba(255, 255, 255, .15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, .15) 50%, rgba(255, 255, 255, .15) 75%, transparent 75%, transparent);
      background-image: linear-gradient(-45deg, rgba(255, 255, 255, .15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, .15) 50%, rgba(255, 255, 255, .15) 75%, transparent 75%, transparent);
      -webkit-background-size: 15px 15px;
      -moz-background-size: 15px 15px;
      -o-background-size: 15px 15px;
      background-size: 15px 15px;
      -webkit-animation: buffering-stripes 2s linear infinite;
      -moz-animation: buffering-stripes 2s linear infinite;
      -ms-animation: buffering-stripes 2s linear infinite;
      -o-animation: buffering-stripes 2s linear infinite;
      animation: buffering-stripes 2s linear infinite
    }

    @-webkit-keyframes buffering-stripes {
      from {
        background-position: 0 0
      }

      to {
        background-position: 30px 0
      }
    }

    @-moz-keyframes buffering-stripes {
      from {
        background-position: 0 0
      }

      to {
        background-position: 30px 0
      }
    }

    @-ms-keyframes buffering-stripes {
      from {
        background-position: 0 0
      }

      to {
        background-position: 30px 0
      }
    }

    @-o-keyframes buffering-stripes {
      from {
        background-position: 0 0
      }

      to {
        background-position: 30px 0
      }
    }

    @keyframes buffering-stripes {
      from {
        background-position: 0 0
      }

      to {
        background-position: 30px 0
      }
    }

    .mejs-controls .mejs-time-rail .mejs-time-loaded {
      background: #3caac8;
      background: rgba(60, 170, 200, .8);
      background: -webkit-gradient(linear, 0 0, 0 100%, from(rgba(44, 124, 145, .8)), to(rgba(78, 183, 212, .8)));
      background: -webkit-linear-gradient(top, rgba(44, 124, 145, .8), rgba(78, 183, 212, .8));
      background: -moz-linear-gradient(top, rgba(44, 124, 145, .8), rgba(78, 183, 212, .8));
      background: -o-linear-gradient(top, rgba(44, 124, 145, .8), rgba(78, 183, 212, .8));
      background: -ms-linear-gradient(top, rgba(44, 124, 145, .8), rgba(78, 183, 212, .8));
      background: linear-gradient(rgba(44, 124, 145, .8), rgba(78, 183, 212, .8));
      width: 0
    }

    .mejs-controls .mejs-time-rail .mejs-time-current {
      background: #fff;
      background: rgba(255, 255, 255, .8);
      background: -webkit-gradient(linear, 0 0, 0 100%, from(rgba(255, 255, 255, .9)), to(rgba(200, 200, 200, .8)));
      background: -webkit-linear-gradient(top, rgba(255, 255, 255, .9), rgba(200, 200, 200, .8));
      background: -moz-linear-gradient(top, rgba(255, 255, 255, .9), rgba(200, 200, 200, .8));
      background: -o-linear-gradient(top, rgba(255, 255, 255, .9), rgba(200, 200, 200, .8));
      background: -ms-linear-gradient(top, rgba(255, 255, 255, .9), rgba(200, 200, 200, .8));
      background: linear-gradient(rgba(255, 255, 255, .9), rgba(200, 200, 200, .8));
      width: 0
    }

    .mejs-controls .mejs-time-rail .mejs-time-handle {
      display: none;
      position: absolute;
      margin: 0;
      width: 10px;
      background: #fff;
      -webkit-border-radius: 5px;
      -moz-border-radius: 5px;
      border-radius: 5px;
      cursor: pointer;
      border: solid 2px #333;
      top: -2px;
      text-align: center
    }

    .mejs-controls .mejs-time-rail .mejs-time-float {
      position: absolute;
      display: none;
      background: #eee;
      width: 36px;
      height: 17px;
      border: solid 1px #333;
      top: -26px;
      margin-left: -18px;
      text-align: center;
      color: #111
    }

    .mejs-controls .mejs-time-rail .mejs-time-float-current {
      margin: 2px;
      width: 30px;
      display: block;
      text-align: center;
      left: 0
    }

    .mejs-controls .mejs-time-rail .mejs-time-float-corner {
      position: absolute;
      display: block;
      width: 0;
      height: 0;
      line-height: 0;
      border: solid 5px #eee;
      border-color: #eee transparent transparent;
      -webkit-border-radius: 0;
      -moz-border-radius: 0;
      border-radius: 0;
      top: 15px;
      left: 13px
    }

    .mejs-long-video .mejs-controls .mejs-time-rail .mejs-time-float {
      width: 48px
    }

    .mejs-long-video .mejs-controls .mejs-time-rail .mejs-time-float-current {
      width: 44px
    }

    .mejs-long-video .mejs-controls .mejs-time-rail .mejs-time-float-corner {
      left: 18px
    }

    .mejs-controls .mejs-fullscreen-button button {
      background-position: -32px 0
    }

    .mejs-controls .mejs-unfullscreen button {
      background-position: -32px -16px
    }

    .mejs-controls .mejs-volume-button {}

    .mejs-controls .mejs-mute button {
      background-position: -16px -16px
    }

    .mejs-controls .mejs-unmute button {
      background-position: -16px 0
    }

    .mejs-controls .mejs-volume-button {
      position: relative
    }

    .mejs-controls .mejs-volume-button .mejs-volume-slider {
      height: 115px;
      width: 25px;
      background: url(https://du11hjcvx0uqb.cloudfront.net/br/dist/webpack-production/703c659e4bf563a05c6338a1727e006c.png);
      background: rgba(50, 50, 50, .7);
      -webkit-border-radius: 0;
      -moz-border-radius: 0;
      border-radius: 0;
      top: -115px;
      left: 0;
      z-index: 1;
      position: absolute;
      margin: 0
    }

    .mejs-controls .mejs-volume-button:hover {
      -webkit-border-radius: 0 0 4px 4px;
      -moz-border-radius: 0 0 4px 4px;
      border-radius: 0 0 4px 4px
    }

    .mejs-controls .mejs-volume-button .mejs-volume-slider .mejs-volume-total {
      position: absolute;
      left: 11px;
      top: 8px;
      width: 2px;
      height: 100px;
      background: #ddd;
      background: rgba(255, 255, 255, .5);
      margin: 0
    }

    .mejs-controls .mejs-volume-button .mejs-volume-slider .mejs-volume-current {
      position: absolute;
      left: 11px;
      top: 8px;
      width: 2px;
      height: 100px;
      background: #ddd;
      background: rgba(255, 255, 255, .9);
      margin: 0
    }

    .mejs-controls .mejs-volume-button .mejs-volume-slider .mejs-volume-handle {
      position: absolute;
      left: 4px;
      top: -3px;
      width: 16px;
      height: 6px;
      background: #ddd;
      background: rgba(255, 255, 255, .9);
      cursor: N-resize;
      -webkit-border-radius: 1px;
      -moz-border-radius: 1px;
      border-radius: 1px;
      margin: 0
    }

    .mejs-controls a.mejs-horizontal-volume-slider {
      height: 26px;
      width: 56px;
      position: relative;
      display: block;
      float: left;
      vertical-align: middle
    }

    .mejs-controls .mejs-horizontal-volume-slider .mejs-horizontal-volume-total {
      position: absolute;
      left: 0;
      top: 11px;
      width: 50px;
      height: 8px;
      margin: 0;
      padding: 0;
      font-size: 1px;
      -webkit-border-radius: 2px;
      -moz-border-radius: 2px;
      border-radius: 2px;
      background: #333;
      background: rgba(50, 50, 50, .8);
      background: -webkit-gradient(linear, 0 0, 0 100%, from(rgba(30, 30, 30, .8)), to(rgba(60, 60, 60, .8)));
      background: -webkit-linear-gradient(top, rgba(30, 30, 30, .8), rgba(60, 60, 60, .8));
      background: -moz-linear-gradient(top, rgba(30, 30, 30, .8), rgba(60, 60, 60, .8));
      background: -o-linear-gradient(top, rgba(30, 30, 30, .8), rgba(60, 60, 60, .8));
      background: -ms-linear-gradient(top, rgba(30, 30, 30, .8), rgba(60, 60, 60, .8));
      background: linear-gradient(rgba(30, 30, 30, .8), rgba(60, 60, 60, .8))
    }

    .mejs-controls .mejs-horizontal-volume-slider .mejs-horizontal-volume-current {
      position: absolute;
      left: 0;
      top: 11px;
      width: 50px;
      height: 8px;
      margin: 0;
      padding: 0;
      font-size: 1px;
      -webkit-border-radius: 2px;
      -moz-border-radius: 2px;
      border-radius: 2px;
      background: #fff;
      background: rgba(255, 255, 255, .8);
      background: -webkit-gradient(linear, 0 0, 0 100%, from(rgba(255, 255, 255, .9)), to(rgba(200, 200, 200, .8)));
      background: -webkit-linear-gradient(top, rgba(255, 255, 255, .9), rgba(200, 200, 200, .8));
      background: -moz-linear-gradient(top, rgba(255, 255, 255, .9), rgba(200, 200, 200, .8));
      background: -o-linear-gradient(top, rgba(255, 255, 255, .9), rgba(200, 200, 200, .8));
      background: -ms-linear-gradient(top, rgba(255, 255, 255, .9), rgba(200, 200, 200, .8));
      background: linear-gradient(rgba(255, 255, 255, .9), rgba(200, 200, 200, .8))
    }

    .mejs-controls .mejs-horizontal-volume-slider .mejs-horizontal-volume-handle {
      display: none
    }

    .mejs-controls .mejs-captions-button {
      position: relative
    }

    .mejs-controls .mejs-captions-button button {
      background-position: -48px 0
    }

    .mejs-controls .mejs-captions-button .mejs-captions-selector {
      position: absolute;
      bottom: 26px;
      right: -51px;
      width: 85px;
      height: 100px;
      background: url(https://du11hjcvx0uqb.cloudfront.net/br/dist/webpack-production/703c659e4bf563a05c6338a1727e006c.png);
      background: rgba(50, 50, 50, .7);
      border: solid 1px transparent;
      padding: 10px 10px 0;
      overflow: hidden;
      -webkit-border-radius: 0;
      -moz-border-radius: 0;
      border-radius: 0
    }

    .mejs-controls .mejs-captions-button .mejs-captions-selector ul {
      margin: 0;
      padding: 0;
      display: block;
      list-style-type: none !important;
      overflow: hidden
    }

    .mejs-controls .mejs-captions-button .mejs-captions-selector ul li {
      margin: 0 0 6px;
      padding: 0;
      list-style-type: none !important;
      display: block;
      color: #fff;
      overflow: hidden
    }

    .mejs-controls .mejs-captions-button .mejs-captions-selector ul li input {
      clear: both;
      float: left;
      margin: 3px 3px 0 5px
    }

    .mejs-controls .mejs-captions-button .mejs-captions-selector ul li label {
      width: 55px;
      float: left;
      padding: 4px 0 0;
      line-height: 15px;
      font-family: Helvetica, Arial, serif;
      font-size: 10px
    }

    .mejs-controls .mejs-captions-button .mejs-captions-translations {
      font-size: 10px;
      margin: 0 0 5px
    }

    .mejs-chapters {
      position: absolute;
      top: 0;
      left: 0;
      border-right: solid 1px #fff;
      width: 10000px;
      z-index: 1
    }

    .mejs-chapters .mejs-chapter {
      position: absolute;
      float: left;
      background: #222;
      background: rgba(0, 0, 0, .7);
      background: -webkit-gradient(linear, 0 0, 0 100%, from(rgba(50, 50, 50, .7)), to(rgba(0, 0, 0, .7)));
      background: -webkit-linear-gradient(top, rgba(50, 50, 50, .7), rgba(0, 0, 0, .7));
      background: -moz-linear-gradient(top, rgba(50, 50, 50, .7), rgba(0, 0, 0, .7));
      background: -o-linear-gradient(top, rgba(50, 50, 50, .7), rgba(0, 0, 0, .7));
      background: -ms-linear-gradient(top, rgba(50, 50, 50, .7), rgba(0, 0, 0, .7));
      background: linear-gradient(rgba(50, 50, 50, .7), rgba(0, 0, 0, .7));
      filter: progid:DXImageTransform.Microsoft.Gradient(GradientType=0, startColorstr=#323232, endColorstr=#000000);
      overflow: hidden;
      border: 0
    }

    .mejs-chapters .mejs-chapter .mejs-chapter-block {
      font-size: 11px;
      color: #fff;
      padding: 5px;
      display: block;
      border-right: solid 1px #333;
      border-bottom: solid 1px #333;
      cursor: pointer
    }

    .mejs-chapters .mejs-chapter .mejs-chapter-block-last {
      border-right: 0
    }

    .mejs-chapters .mejs-chapter .mejs-chapter-block:hover {
      background: #666;
      background: rgba(102, 102, 102, .7);
      background: -webkit-gradient(linear, 0 0, 0 100%, from(rgba(102, 102, 102, .7)), to(rgba(50, 50, 50, .6)));
      background: -webkit-linear-gradient(top, rgba(102, 102, 102, .7), rgba(50, 50, 50, .6));
      background: -moz-linear-gradient(top, rgba(102, 102, 102, .7), rgba(50, 50, 50, .6));
      background: -o-linear-gradient(top, rgba(102, 102, 102, .7), rgba(50, 50, 50, .6));
      background: -ms-linear-gradient(top, rgba(102, 102, 102, .7), rgba(50, 50, 50, .6));
      background: linear-gradient(rgba(102, 102, 102, .7), rgba(50, 50, 50, .6));
      filter: progid:DXImageTransform.Microsoft.Gradient(GradientType=0, startColorstr=#666666, endColorstr=#323232)
    }

    .mejs-chapters .mejs-chapter .mejs-chapter-block .ch-title {
      font-size: 12px;
      font-weight: 700;
      display: block;
      white-space: nowrap;
      text-overflow: ellipsis;
      margin: 0 0 3px;
      line-height: 12px
    }

    .mejs-chapters .mejs-chapter .mejs-chapter-block .ch-timespan {
      font-size: 12px;
      line-height: 12px;
      margin: 3px 0 4px;
      display: block;
      white-space: nowrap;
      text-overflow: ellipsis
    }

    .mejs-captions-layer {
      position: absolute;
      bottom: 0;
      left: 0;
      text-align: center;
      line-height: 20px;
      font-size: 16px;
      color: #fff
    }

    .mejs-captions-layer a {
      color: #fff;
      text-decoration: underline
    }

    .mejs-captions-layer[lang=ar] {
      font-size: 20px;
      font-weight: 400
    }

    .mejs-captions-position {
      position: absolute;
      width: 100%;
      bottom: 15px;
      left: 0
    }

    .mejs-captions-position-hover {
      bottom: 35px
    }

    .mejs-captions-text,
    .mejs__captions-text * {
      padding: 0;
      background: url(https://du11hjcvx0uqb.cloudfront.net/br/dist/webpack-production/703c659e4bf563a05c6338a1727e006c.png);
      background: rgba(20, 20, 20, .5);
      white-space: pre-wrap;
      -webkit-box-shadow: 5px 0 0 rgba(20, 20, 20, .5), -5px 0 0 rgba(20, 20, 20, .5);
      box-shadow: 5px 0 0 rgba(20, 20, 20, .5), -5px 0 0 rgba(20, 20, 20, .5)
    }

    .me-cannotplay {}

    .me-cannotplay a {
      color: #fff;
      font-weight: 700
    }

    .me-cannotplay span {
      padding: 15px;
      display: block
    }

    .mejs-controls .mejs-loop-off button {
      background-position: -64px -16px
    }

    .mejs-controls .mejs-loop-on button {
      background-position: -64px 0
    }

    .mejs-controls .mejs-backlight-off button {
      background-position: -80px -16px
    }

    .mejs-controls .mejs-backlight-on button {
      background-position: -80px 0
    }

    .mejs-controls .mejs-picturecontrols-button {
      background-position: -96px 0
    }

    .mejs-contextmenu {
      position: absolute;
      width: 150px;
      padding: 10px;
      border-radius: 4px;
      top: 0;
      left: 0;
      background: #fff;
      border: solid 1px #999;
      z-index: 1001
    }

    .mejs-contextmenu .mejs-contextmenu-separator {
      height: 1px;
      font-size: 0;
      margin: 5px 6px;
      background: #333
    }

    .mejs-contextmenu .mejs-contextmenu-item {
      font-family: Helvetica, Arial, serif;
      font-size: 12px;
      padding: 4px 6px;
      cursor: pointer;
      color: #333
    }

    .mejs-contextmenu .mejs-contextmenu-item:hover {
      background: #2C7C91;
      color: #fff
    }

    .mejs-controls .mejs-sourcechooser-button {
      position: relative
    }

    .mejs-controls .mejs-sourcechooser-button button {
      background-position: -128px 0
    }

    .mejs-controls .mejs-sourcechooser-button .mejs-sourcechooser-selector {
      position: absolute;
      bottom: 26px;
      right: -10px;
      width: 130px;
      height: 100px;
      background: url(https://du11hjcvx0uqb.cloudfront.net/br/dist/webpack-production/703c659e4bf563a05c6338a1727e006c.png);
      background: rgba(50, 50, 50, .7);
      border: solid 1px transparent;
      padding: 10px;
      overflow: hidden;
      -webkit-border-radius: 0;
      -moz-border-radius: 0;
      border-radius: 0
    }

    .mejs-controls .mejs-sourcechooser-button .mejs-sourcechooser-selector ul {
      margin: 0;
      padding: 0;
      display: block;
      list-style-type: none !important;
      overflow: hidden
    }

    .mejs-controls .mejs-sourcechooser-button .mejs-sourcechooser-selector ul li {
      margin: 0 0 6px;
      padding: 0;
      list-style-type: none !important;
      display: block;
      color: #fff;
      overflow: hidden
    }

    .mejs-controls .mejs-sourcechooser-button .mejs-sourcechooser-selector ul li input {
      clear: both;
      float: left;
      margin: 3px 3px 0 5px
    }

    .mejs-controls .mejs-sourcechooser-button .mejs-sourcechooser-selector ul li label {
      width: 100px;
      float: left;
      padding: 4px 0 0;
      line-height: 15px;
      font-family: Helvetica, Arial, serif;
      font-size: 10px
    }

    .mejs-postroll-layer {
      position: absolute;
      bottom: 0;
      left: 0;
      width: 100%;
      height: 100%;
      background: url(https://du11hjcvx0uqb.cloudfront.net/br/dist/webpack-production/703c659e4bf563a05c6338a1727e006c.png);
      background: rgba(50, 50, 50, .7);
      z-index: 1000;
      overflow: hidden
    }

    .mejs-postroll-layer-content {
      width: 100%;
      height: 100%
    }

    .mejs-postroll-close {
      position: absolute;
      right: 0;
      top: 0;
      background: url(https://du11hjcvx0uqb.cloudfront.net/br/dist/webpack-production/703c659e4bf563a05c6338a1727e006c.png);
      background: rgba(50, 50, 50, .7);
      color: #fff;
      padding: 4px;
      z-index: 100;
      cursor: pointer
    }

    div.mejs-speed-button {
      width: 46px !important;
      position: relative
    }

    .mejs-controls .mejs-button.mejs-speed-button button {
      background: transparent;
      width: 36px;
      font-size: 11px;
      line-height: normal;
      color: #fff
    }

    .mejs-controls .mejs-speed-button .mejs-speed-selector {
      position: absolute;
      top: -100px;
      left: -10px;
      width: 60px;
      height: 100px;
      background: url(https://du11hjcvx0uqb.cloudfront.net/br/dist/webpack-production/703c659e4bf563a05c6338a1727e006c.png);
      background: rgba(50, 50, 50, .7);
      border: solid 1px transparent;
      padding: 0;
      overflow: hidden;
      -webkit-border-radius: 0;
      -moz-border-radius: 0;
      border-radius: 0
    }

    .mejs-controls .mejs-speed-button .mejs-speed-selector ul li label.mejs-speed-selected {
      color: rgba(33, 248, 248, 1)
    }

    .mejs-controls .mejs-speed-button .mejs-speed-selector ul {
      margin: 0;
      padding: 0;
      display: block;
      list-style-type: none !important;
      overflow: hidden
    }

    .mejs-controls .mejs-speed-button .mejs-speed-selector ul li {
      margin: 0 0 6px;
      padding: 0 10px;
      list-style-type: none !important;
      display: block;
      color: #fff;
      overflow: hidden
    }

    .mejs-controls .mejs-speed-button .mejs-speed-selector ul li input {
      clear: both;
      float: left;
      margin: 3px 3px 0 5px
    }

    .mejs-controls .mejs-speed-button .mejs-speed-selector ul li label {
      width: 60px;
      float: left;
      padding: 4px 0 0;
      line-height: 15px;
      font-family: Helvetica, Arial, serif;
      font-size: 11px;
      color: #fff;
      margin-left: 5px;
      cursor: pointer
    }

    .mejs-controls .mejs-speed-button .mejs-speed-selector ul li:hover {
      background-color: #c8c8c8 !important;
      background-color: rgba(255, 255, 255, .4) !important
    }

    .mejs-controls .mejs-button.mejs-jump-forward-button {
      background: transparent url(https://du11hjcvx0uqb.cloudfront.net/br/dist/webpack-production/15e1ac8cbacc2efdf1ac2677de48a253.png) no-repeat 3px 3px
    }

    .mejs-controls .mejs-button.mejs-jump-forward-button button {
      background: transparent;
      font-size: 9px;
      line-height: normal;
      color: #fff
    }

    .mejs-controls .mejs-button.mejs-skip-back-button {
      background: transparent url(https://du11hjcvx0uqb.cloudfront.net/br/dist/webpack-production/cd6dc830eb45b3a5a96bbc936ff54846.png) no-repeat 3px 3px
    }

    .mejs-controls .mejs-button.mejs-skip-back-button button {
      background: transparent;
      font-size: 9px;
      line-height: normal;
      color: #fff
    }
  </style>
  <style type="text/css">
    /*
 * Copyright (C) 2014 - present Instructure, Inc.
 *
 * This file is part of Canvas.
 *
 * Canvas is free software: you can redistribute it and/or modify it under
 * the terms of the GNU Affero General Public License as published by the Free
 * Software Foundation, version 3 of the License.
 *
 * Canvas is distributed in the hope that it will be useful, but WITHOUT ANY
 * WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR
 * A PARTICULAR PURPOSE. See the GNU Affero General Public License for more
 * details.
 *
 * You should have received a copy of the GNU Affero General Public License along
 * with this program. If not, see <http://www.gnu.org/licenses/>.
 */

    /* customizations to mediaelementplayer css */

    /*
  Because this file is not proccessed by our brandable_css sass pipeline,
  it cannot use our sass-direction helpers. So we have to handle RTL manually
  by putting direction-specific styles in [dir="ltr"] or [dir="rtl"] blocks.
*/
    /* stylelint-disable property-disallowed-list, declaration-property-value-disallowed-list */

    /* good menu widths */
    .mejs-sourcechooser-selector {
      width: 160px;
    }

    .mejs-sourcechooser-selector label {
      width: 160px !important;
    }

    .mejs-captions-selector {
      width: 105px;
    }

    [dir="ltr"] .mejs-captions-selector {
      text-align: left
    }

    [dir="rtl"] .mejs-captions-selector {
      text-align: right
    }

    .mejs-captions-selector label {
      width: 70px !important;
    }

    /* Subtitile upload link */
    .mejs-captions-selector .upload-track {
      color: white;
      margin-top: 3px;
      margin-bottom: 5px;
    }

    [dir="ltr"] .mejs-captions-selector .upload-track {
      margin-right: 0px;
      margin-left: 5px;
      float: left;
    }

    [dir="rtl"] .mejs-captions-selector .upload-track {
      margin-left: 0px;
      margin-right: 5px;
      float: right;
    }

    /* "x" button to remove a subtitle */
    .mejs-captions-selector a[data-remove] {
      position: absolute;
      top: 0;
      color: white;
    }

    [dir="ltr"] .mejs-captions-selector a[data-remove] {
      right: 0
    }

    [dir="rtl"] .mejs-captions-selector a[data-remove] {
      left: 0
    }


    /* style menu items without a radio button */
    .mejs-button [role="menu"] {
      padding: 0 !important;
    }

    /* compensate for above 0 padding */
    .mejs-button [role="menu"] ul li {
      position: relative;
      padding: 0 10px !important;
    }

    /* add a hover effect */
    .mejs-button [role="menu"] ul li:hover {
      background-color: #c8c8c8 !important;
      background-color: rgba(255, 255, 255, 0.4) !important;
    }

    .mejs-button [role="menu"] ul li input {
      border: 0;
      clip: rect(0 0 0 0);
      position: absolute;
      overflow: hidden;
      margin: -1px;
      padding: 0;
      width: 1px;
      height: 1px;
    }

    .mejs-button [role="menu"] ul li label {
      cursor: pointer;
    }

    [dir="ltr"] .mejs-button [role="menu"] ul li label {
      margin-left: 5px
    }

    [dir="rtl"] .mejs-button [role="menu"] ul li label {
      margin-right: 5px
    }


    .mejs-button [role="menu"] label.mejs-selected {
      color: #21f8f8 !important;
    }

    /* stylelint-enable property-disallowed-list, declaration-property-value-disallowed-list */
  </style>
  <script charset="utf-8"
    src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/263-c-987b5107e8.js"></script>
  <script charset="utf-8"
    src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/691-c-800e12c5fc.js"></script>
  <script type="text/javascript" src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/loader.js"></script>
  <script type="text/javascript"
    src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/require-with-callback.jsp"></script>
  <script type="text/javascript" charset="utf-8" async="" data-requirecontext="eesy" data-requiremodule="sessionInfo"
    src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/sessionInfo.js"></script>
  <script type="text/javascript" charset="utf-8" async="" data-requirecontext="eesy" data-requiremodule="engine_core"
    src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/engine_core.js"></script>
  <script type="text/javascript" charset="utf-8" async="" data-requirecontext="eesy" data-requiremodule="jquery-private"
    src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/jquery-private.jsp"></script>
  <script type="text/javascript" charset="utf-8" async="" data-requirecontext="eesy" data-requiremodule="context-probe"
    src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/context-probe.js"></script>
  <script type="text/javascript" charset="utf-8" async="" data-requirecontext="eesy"
    data-requiremodule="context-handling"
    src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/context-handling.js"></script>
  <script type="text/javascript" charset="utf-8" async="" data-requirecontext="eesy"
    data-requiremodule="monitor-handling"
    src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/monitor-handling.js"></script>
  <script type="text/javascript" charset="utf-8" async="" data-requirecontext="eesy"
    data-requiremodule="events-urlchange"
    src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/events-urlchange.js"></script>
  <script type="text/javascript" charset="utf-8" async="" data-requirecontext="eesy"
    data-requiremodule="events-domchange"
    src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/events-domchange.js"></script>
  <script type="text/javascript" charset="utf-8" async="" data-requirecontext="eesy" data-requiremodule="events-iframe"
    src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/events-iframe.js"></script>
  <script type="text/javascript" charset="utf-8" async="" data-requirecontext="eesy" data-requiremodule="engine-state"
    src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/engine-state.js"></script>
  <script type="text/javascript" charset="utf-8" async="" data-requirecontext="eesy" data-requiremodule="keep-alive"
    src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/keep-alive.js"></script>
  <script type="text/javascript" charset="utf-8" async="" data-requirecontext="eesy" data-requiremodule="presentation"
    src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/presentation.js"></script>
  <script type="text/javascript" charset="utf-8" async="" data-requirecontext="eesy"
    data-requiremodule="found-items-handler"
    src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/found-items-handler.js"></script>
  <script type="text/javascript" charset="utf-8" async="" data-requirecontext="eesy" data-requiremodule="mouse"
    src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/mouse.js"></script>
  <script type="text/javascript" charset="utf-8" async="" data-requirecontext="eesy"
    data-requiremodule="context-tree-matcher"
    src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/context-tree-matcher.js"></script>
  <script type="text/javascript" charset="utf-8" async="" data-requirecontext="eesy" data-requiremodule="eesy-timers"
    src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/eesy-timers.js"></script>
  <script type="text/javascript" charset="utf-8" async="" data-requirecontext="eesy" data-requiremodule="hints"
    src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/hints.js"></script>
  <script type="text/javascript" charset="utf-8" async="" data-requirecontext="eesy"
    data-requiremodule="helpitem-handling"
    src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/helpitem-handling.js"></script>
  <script type="text/javascript" charset="utf-8" async="" data-requirecontext="eesy" data-requiremodule="support-tab"
    src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/support-tab.js"></script>
  <script type="text/javascript" charset="utf-8" async="" data-requirecontext="eesy" data-requiremodule="utils"
    src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/utils.js"></script>
  <script type="text/javascript" charset="utf-8" async="" data-requirecontext="eesy" data-requiremodule="session-events"
    src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/session-events.js"></script>
  <script type="text/javascript" charset="utf-8" async="" data-requirecontext="eesy"
    data-requiremodule="iframe_communicator_server"
    src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/iframe_communicator_server.js"></script>
  <link rel="stylesheet" type="text/css"
    href="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/canvas.459708bb99eda9c9799f4c9f0d163353.css">
  <script type="text/javascript" charset="utf-8" async="" data-requirecontext="eesy" data-requiremodule="context-links"
    src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/context-links.js"></script>
  <script type="text/javascript" charset="utf-8" async="" data-requirecontext="eesy" data-requiremodule="json"
    src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/json.js"></script>
  <script type="text/javascript" charset="utf-8" async="" data-requirecontext="eesy"
    data-requiremodule="helpitem-loader"
    src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/helpitem-loader.js"></script>
  <script type="text/javascript" charset="utf-8" async="" data-requirecontext="eesy" data-requiremodule="mustachejs"
    src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/mustachejs-private.jsp"></script>
  <script type="text/javascript" charset="utf-8" async="" data-requirecontext="eesy"
    data-requiremodule="proactive-hints"
    src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/proactive-hints.js"></script>
  <script type="text/javascript" charset="utf-8" async="" data-requirecontext="eesy"
    data-requiremodule="presentation-helper"
    src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/presentation-helper.js"></script>
  <script type="text/javascript" charset="utf-8" async="" data-requirecontext="eesy"
    data-requiremodule="helpitem-visibility"
    src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/helpitem-visibility.js"></script>
  <script type="text/javascript" charset="utf-8" async="" data-requirecontext="eesy" data-requiremodule="systrays"
    src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/systrays.js"></script>
  <script type="text/javascript" charset="utf-8" async="" data-requirecontext="eesy" data-requiremodule="quick-survey"
    src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/quick-survey.js"></script>
  <script type="text/javascript" charset="utf-8" async="" data-requirecontext="eesy" data-requiremodule="popups"
    src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/popups.js"></script>
  <script type="text/javascript" charset="utf-8" async="" data-requirecontext="eesy"
    data-requiremodule="condition-matcher"
    src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/condition-matcher.js"></script>
  <script type="text/javascript" charset="utf-8" async="" data-requirecontext="eesy"
    data-requiremodule="view-controller"
    src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/view-controller.js"></script>
  <script type="text/javascript" charset="utf-8" async="" data-requirecontext="eesy"
    data-requiremodule="helpitem-handlers"
    src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/helpitem-handlers.js"></script>
  <script type="text/javascript" charset="utf-8" async="" data-requirecontext="eesy" data-requiremodule="text"
    src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/text.js"></script>
  <link type="text/css" href="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/style.css" rel="stylesheet">
  <link type="text/css" href="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/supportCenter.min.css"
    rel="stylesheet">
  <link type="text/css" href="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/override-proactive.css"
    rel="stylesheet">
  <link type="text/css"
    href="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/support-center-theme-defaults.min.css"
    rel="stylesheet">
</head>

<body
  class="with-left-side course-menu-expanded padless-content pages primary-nav-expanded full-width context-course_639034 responsive_awareness responsive_misc webkit chrome no-touch show">
  <!-- Google Tag Manager (noscript) --><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-PK3D3GP"
      height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
  <!-- End Google Tag Manager (noscript) -->

  <noscript>
    <div role="alert" class="ic-flash-static ic-flash-error">
      <div class="ic-flash__icon" aria-hidden="true">
        <i class="icon-warning"></i>
      </div>
      <h1>You need to have JavaScript enabled in order to access this site.</h1>
    </div>
  </noscript>




  <ul id="flash_message_holder"></ul>
  <div id="flash_screenreader_holder" role="alert" aria-live="assertive" aria-relevant="additions"
    class="screenreader-only" aria-atomic="false"></div>

  <div id="application" class="ic-app">



    <div id="instructure_ajax_error_box">
      <div style="text-align: right; background-color: #fff;"><a
          href="https://usu.instructure.com/courses/639034/pages/jake-cooks-portfolio#"
          class="close_instructure_ajax_error_box_link">Close</a></div>
      <iframe id="instructure_ajax_error_result"
        src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/saved_resource.html" style="border: 0;"
        title="Error"></iframe>
    </div>

    <div id="wrapper" class="ic-Layout-wrapper">
      <div id="main" class="ic-Layout-columns">
        <div class="ic-Layout-watermark"></div>
        <div id="not_right_side" class="ic-app-main-content">
          <div id="content-wrapper" class="ic-Layout-contentWrapper">

            <div id="content" class="ic-Layout-contentMain" role="main">
              <div id="wiki_page_show">
                <div class="header-bar-outer-container">
                  <div class="sticky-toolbar sticky" data-sticky="">
                  </div>
                  <div class="page-changed-alert" role="alert" aria-atomic="true" aria-live="polite"></div>
                </div>

                <div id="direct-share-mount-point"></div>


                <div class="show-content user_content clearfix enhanced">
                  <h1 class="page-title">Jake Cook's Portfolio<button class="ally-accessible-versions"
                      data-id="page:1590446" data-ally-content-id="page:1590446"
                      data-ally-richcontent-eid="page:1590446">
                      <span class="ally-prominent-af-download-button" title="Alternative formats"></span>
                    </button></h1>


                  <p><span style="font-size: 24px;"><strong>Plant Classification with Neural Networks</strong></span>
                  </p>
                  <p><span style="font-size: 14pt;"><span style="text-decoration: underline;">Completed
                        Objectives:</span></span></p>
                  <ul>
                    <li><span style="font-size: 12pt;">Running notebook with existing keras CNN (97%+ accuracy): <a
                          class="inline_disabled external" href="https://www.kaggle.com/limitpointinf0/crop-vs-weeds"
                          target="_blank"
                          rel="noreferrer noopener"><span>https://www.kaggle.com/limitpointinf0/crop-vs-weeds</span><span
                            aria-hidden="true" class="ui-icon ui-icon-extlink ui-icon-inline"
                            title="Links to an external site."></span><span class="screenreader-only">&nbsp;(Links to an
                            external site.)</span></a></span></li>
                    <li><span style="font-size: 12pt;">Saving and loading trained models</span></li>
                    <li><span style="font-size: 12pt;">Testing prediction capability</span></li>
                    <li><span style="font-size: 12pt;">Training visuals</span></li>
                    <li>Integrating additional data -- continuous effort</li>
                    <li>Confusion matrices</li>
                    <li>Multi-model comparisons for overfitting determination</li>
                  </ul>
                  <p><span style="text-decoration: underline;"><strong><span
                          style="font-size: 18pt;">Motivations</span></strong></span></p>
                  <p>This project aims to understand and reduce carbon emissions in one sector of one market which
                    contributes moderately to global warming: Agricultural pesticide usage. Agriculture contributes over
                    14% of all greenhouse gas emissions produced globally, where 2% of the global emissions is due
                    directly to nitrogen fertilizer synthesizing and use and other agri-chemicals. Additionally,
                    pesticides are in heavy use to reduce crop competition with local or invasive species. The
                    combination of these items increases soil and ecosystem degradation, and leeches chemicals into
                    waterways and local communities.</p>
                  <p>These problems are rectifiable with adoption of precision agriculture, which meshes modern
                    technology, farmers, and plants for optimal and efficient use of lands, materials, and time to
                    increase yield while decreasing costs. In particular, the pesticide issue can be solved directly by
                    integrating a machine intelligence model with a mechanical weeding or a precision spot sprayer
                    component to minimize or eliminate pesticide chemical use. In addition, reducing the soil load by
                    removing competing plants potentially allows both higher yields and fewer soil maintenance steps
                    such as nitrogen injection, particularly when coupled with smart crop rotation and other sustainable
                    farming practices.</p>
                  <p><span style="text-decoration: underline; font-size: 18pt;"><strong>Problem</strong></span></p>
                  <p><span style="font-size: 12pt;">For an image of a small plot of cropland (no larger than a few feet
                      on a side), identify the crops, in seedling growth stage, and differentiate from weeds which may
                      or may not be present in each image. Determine the positions of the weeds to be mechanically
                      removed or sprayed.</span></p>
                  <p><span style="text-decoration: underline; font-size: 18pt;"><strong>Solution
                        Methodology</strong></span></p>
                  <p>The methods for developing a solution to this problem of plant localization and identification
                    involves the following components:</p>
                  <ul>
                    <li>Data sets of images of plant seedlings or crops which have not reached maturity</li>
                    <li>An image classification model
                      <ul>
                        <li>Verification of model</li>
                        <li>Identify weaknesses, limitations, etc</li>
                      </ul>
                    </li>
                    <li>Object detection model
                      <ul>
                        <li>Object localization</li>
                        <li>Isolated image fed into classifier</li>
                      </ul>
                    </li>
                  </ul>
                  <p><strong>Datasets</strong></p>
                  <p>Object classification from images, when not distinctly unique, requires a large amount of training
                    and sample data to train a model with. This was found to be particularly important for identifying
                    and classifying unique plants.</p>
                  <p>The following datasets were used as the primary source to train a multi-class Convolutional Neural
                    Network (CNN) model:</p>
                  <ul>
                    <li><a href="https://www.kaggle.com/c/plant-seedlings-classification/data" target="_blank"
                        class="external"
                        rel="noreferrer noopener"><span>https://www.kaggle.com/c/plant-seedlings-classification/data</span><span
                          aria-hidden="true" class="ui-icon ui-icon-extlink ui-icon-inline"
                          title="Links to an external site."></span><span class="screenreader-only">&nbsp;(Links to an
                          external site.)</span></a></li>
                    <li><a href="https://www.kaggle.com/vbookshelf/v2-plant-seedlings-dataset" target="_blank"
                        class="external"
                        rel="noreferrer noopener"><span>https://www.kaggle.com/vbookshelf/v2-plant-seedlings-dataset</span><span
                          aria-hidden="true" class="ui-icon ui-icon-extlink ui-icon-inline"
                          title="Links to an external site."></span><span class="screenreader-only">&nbsp;(Links to an
                          external site.)</span></a></li>
                    <li><a href="https://www.kaggle.com/tobiasnw/dandelion-image-classifier" target="_blank"
                        class="external"
                        rel="noreferrer noopener"><span>https://www.kaggle.com/tobiasnw/dandelion-image-classifier</span><span
                          aria-hidden="true" class="ui-icon ui-icon-extlink ui-icon-inline"
                          title="Links to an external site."></span><span class="screenreader-only">&nbsp;(Links to an
                          external site.)</span></a></li>
                  </ul>
                  <p>Additional datasets were investigated and examined, covering various crops, weeds, plant ages, and
                    more, but were not included if the images did not fit reasonably in most of the following
                    categories:</p>
                  <ul>
                    <li>Images containing only plants of the same type</li>
                    <li>Quality sufficiently high to determine details between species</li>
                    <li>Common crops in seedling or early growth stages or weeds/invasive species</li>
                    <li>Sufficient data to train and test on, no less the ~250-300 images</li>
                  </ul>
                  <p>This focus on quality and seedlings age range for crops limited the available datasets for
                    integration. A large majority of datasets available were of common plants, but were focused on later
                    growth stages and larger field of views (entire fields) rather than data as would be collected
                    on-machine with a precision agriculture method. Further, mixed plant types are not permissible for
                    training a classifier as categories would be easily confused by the trained model and to manually
                    sort or modify the images in usable quantities would take more time than was viable in a single
                    semester's focus. The above linked datasets proved sufficient for determining the viability and
                    practicality of training a CNN for classification of multiple types of plants.</p>
                  <p><strong>Convolutional Neural&nbsp;</strong><strong>Network</strong></p>
                  <p>To achieve the goal of multi-object classification, a Convolutional Neural Network (CNN) is seen as
                    the best approach. CNN's are practical for image related tasks because of their input processing
                    methodology, which breaks apart images, audio, or more into defining characteristics at several
                    detail levels. There are three main types of layers in a CNN, convolutional layers, pooling layers,
                    and the fully-connected (FC) layer. Keras, a subset of Tensorflow which allows for easier model
                    creation and manipulation, was used to create and test a CNN for this project.</p>
                  <p>The convolutional layer works by taking a multidimensional input image, namely one with a height,
                    width, and depth (colors) and filtering pixels of the image through a feature mapping layer. The
                    number of filters applied changes the depth of the output array, and image arrays may be filtered
                    without iterating over every pixel, which may also be padded to ensure the entire image is processed
                    without negating fringe pixels, such as the bottom and right&nbsp; two rows &amp; columns of pixels
                    below, where the sampling method is a 3x3 matrix.</p>
                  <p><img
                      src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/ICLH_Diagram_Batch_02_17A-ConvolutionalNeuralNetworks-WHITEBG.png"
                      alt="matrix multiplication in convolutional neural networks" width="648" height="377"
                      style="max-width: 648px;" data-id="2021"></p>
                  <p>&nbsp;</p>
                  <p>The pooling layer is used alongside the convolutional layer and down samples the input image array
                    using either maximum value of average value pooling. This process is useful for two reasons: first
                    it can reduce the detail to allow for new features to be determined, for example on a plant a down
                    sample may help to indicate the width and shape of a leaf, whereas the original input would allow
                    the notifiable features of ridges, edge details, and leaf patterns such as veins, blemishes or
                    spots.</p>
                  <p>Lastly the fully connected layer takes the outputs of the previous two layers and assigns a
                    probable class value to the source (usually between 0 and 1), which can map to a name. This layer
                    also connects each node to every other node in the previous layer, unlike the previous layers which
                    tend to be partially connected instead.</p>
                  <p>For classifying plants, a kaggle competition notebook was used as a base starting point, <span
                      style="font-size: 12pt;"><a href="https://www.kaggle.com/limitpointinf0/crop-vs-weeds"
                        target="_blank" class="external"
                        rel="noreferrer noopener"><span>https://www.kaggle.com/limitpointinf0/crop-vs-weeds</span><span
                          aria-hidden="true" class="ui-icon ui-icon-extlink ui-icon-inline"
                          title="Links to an external site."></span><span class="screenreader-only">&nbsp;(Links to an
                          external site.)</span></a>. This notebook provided an image pre-processing method, simple
                      dataset handling, and a training model. These components were valuable in creating an initial
                      benchmark to work from on the V1 plant seedling dataset. This exact code was compared against the
                      V2 plant seedling dataset to demonstrate the value increase in obtaining and training on more data
                      (about 10-15% increase in image count).</span></p>
                  <p><span style="font-size: 12pt;">The following points highlight the modifications made to develop and
                      understand a classification model:</span></p>
                  <ol>
                    <li><span style="font-size: 12pt;">Add multi-dataset handling for side-by-side comparisons</span>
                    </li>
                    <li><span style="font-size: 12pt;">Develop a predication process to validate the trained model on
                        both previously seen and unseen images</span></li>
                    <li><span style="font-size: 12pt;">Increase data handling process for more rapid comparison of
                        datasets, training procedures, and more</span></li>
                    <li><span style="font-size: 12pt;">Save models and transformed image arrays</span></li>
                    <li><span style="font-size: 12pt;">Develop visualizations and graphics of data training,
                        verification, and prediction results</span></li>
                  </ol>
                  <p><strong><span style="font-size: 12pt;">Object detection</span></strong></p>
                  <p><span style="font-size: 12pt;">Object detection is the process of isolating multiple objects within
                      an single image. In this problem of precision agriculture, that includes determining plant from
                      soil/ground from weeds to be able to feed into a classifier independently and determine the type
                      of plant in focus. Keras provides an extensive system for object detection called RetinaNet.
                      RetinaNet is a single stage object detector which is fast and accurate enough for many cases,
                      including this project as the entire object detection and classification process must occur
                      real-time on a moving machine. RetinaNet employs a feature pyramid to distinguish objects using a
                      loss function called Focal loss, which is designed to reduce issues with foreground/background
                      processing balances.</span></p>
                  <p><span style="font-size: 12pt;">RetinaNet is best used as directly as an object detector feeding
                      into a classifier in one process. This limits the usability of the previous CNN model as it is
                      developed as a stand-alone classifier which could be ported for use in applications or other
                      software. Furthermore, object detection is a large field with many components, which as discussed
                      below require far more time, testing, data, and verification than was available originally in this
                      project.</span></p>
                  <p><span style="font-size: 12pt;">Some of the key components of object detection with RetinaNet
                      include:</span></p>
                  <ul>
                    <li><span style="font-size: 12pt;">Constructing a feature pyramid scheme</span></li>
                    <li><span style="font-size: 12pt;">Building the bounding boxes for objects</span>
                      <ul>
                        <li><span style="font-size: 12pt;">center point bounding box</span></li>
                        <li><span style="font-size: 12pt;">corner-defined bounding box</span></li>
                      </ul>
                    </li>
                    <li><span style="font-size: 12pt;">Integrating a classifier</span></li>
                    <li><span style="font-size: 12pt;">Building a detection handler for feeding data into the RetinaNet
                        model</span></li>
                  </ul>
                  <p>&nbsp;</p>
                  <p><span style="text-decoration: underline;"><strong><span
                          style="font-size: 18pt;">Results</span></strong></span></p>
                  <p><span style="font-size: 12pt;">During and after implementation of the classification model
                      components, several visualizations and data insights were created.</span></p>
                  <p><span style="font-size: 12pt;">The following charts show training and validation accuracy and loss
                      values over 25 epochs on both versions of the seedling datasets, on a batch size of 10.</span></p>
                  <p><img src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/20_epoch_train_v1.png"
                      alt="20_epoch_train_v1.png" width="650" height="434"
                      data-api-endpoint="https://usu.instructure.com/api/v1/users/1298792/files/81034319"
                      data-api-returntype="File" data-id="81034319" style="max-width: 650px;"> &nbsp;<img
                      src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/20_epoch_train_v2.png"
                      alt="20_epoch_train_v2.png" width="651" height="433"
                      data-api-endpoint="https://usu.instructure.com/api/v1/users/1298792/files/81034329"
                      data-api-returntype="File" data-id="81034329" style="max-width: 651px;">&nbsp;&nbsp;</p>
                  <p>By examining the loss values on each, we can realize overfitting levels and determine the
                    approximate best number of epochs to train a final model. Note that in these two training models,
                    the V2 validation loss, despite increasing over the remaining 20 epochs, remains lower than the V1
                    validation loss. This is attributed to the approximate 10% increase in images available in the V2
                    dataset, which allows for slightly longer training without as much risk of memorizing the specific
                    features of the images available.</p>
                  <p>Using this information a final model of each dataset version was trained to 25 epochs with a batch
                    size 20. This batch size was chosen due to the increase in images available by incorporating the
                    complete dataset into the training, allowing more training weight updates in general, while also
                    slightly reducing the number of weight modifications to prevent overfitting on this specific data.
                    This final model for the V2 dataset training appeared as follows:</p>
                  <p><img
                      src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/final-training-model_v2.jpg"
                      alt="final-training-model_v2.jpg" width="712" height="356"
                      data-api-endpoint="https://usu.instructure.com/api/v1/users/1298792/files/81034406"
                      data-api-returntype="File" data-id="81034406" style="max-width: 712px;">&nbsp;&nbsp;</p>
                  <p>As can be seen, this model trains to a predicted accuracy of 99+% and a near 0 loss. While this
                    looks fantastic, it requires an examination of actual predictions to indicate if there is any true
                    level of accuracy. This was done using hand selected images sampled from both the V2 dataset trained
                    on and various images found on the internet. The results of the predictions are indicated in the
                    following heatmap as a confusion matrix</p>
                  <p><img
                      src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/prediction_heatmap_v2_model.png"
                      alt="prediction_heatmap_v2_model.png" width="587" height="484"
                      data-api-endpoint="https://usu.instructure.com/api/v1/users/1298792/files/81034503"
                      data-api-returntype="File" data-id="81034503" style="max-width: 587px;"> &nbsp;&nbsp;&nbsp;</p>
                  <p>As seen in this heatmap, several categories are confused in this model. Many of the "confused"
                    classifications were predictions on images sourced from the internet, not trained on and not seen
                    before. Common Wheat is confused most often as Black grass. Further, Black grass is also confused as
                    Maize (corn) in a surprisingly high frequency, alongside several other confusions. This latter part
                    is surprising as maize seedlings tend to look unique (see annotated images below). Here one can
                    examine why Wheat, Black grass and Silky bent (pictured in said order) may often be confused for
                    each other, even to a human observer:</p>
                  <p><img src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/wheat.png" alt="wheat.png"
                      width="300" height="300"
                      data-api-endpoint="https://usu.instructure.com/api/v1/users/1298792/files/81034565"
                      data-api-returntype="File" data-id="81034565" style="max-width: 300px;"> &nbsp;<img
                      src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/black-grass.png"
                      alt="black-grass.png" width="283" height="300"
                      data-api-endpoint="https://usu.instructure.com/api/v1/users/1298792/files/81034572"
                      data-api-returntype="File" data-id="81034572" style="max-width: 283px;"> &nbsp;<img
                      src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/loose silky bent.png"
                      alt="loose silky bent.png" width="300" height="300"
                      data-api-endpoint="https://usu.instructure.com/api/v1/users/1298792/files/81034576"
                      data-api-returntype="File" data-id="81034576" style="max-width: 300px;">&nbsp;&nbsp;</p>
                  <p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Wheat&nbsp; &nbsp;
                    &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
                    &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
                    &nbsp; &nbsp; &nbsp;Black Grass&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
                    &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
                    &nbsp; &nbsp; &nbsp;Loose Silky Bent</p>
                  <p>&nbsp;</p>
                  <p><img src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/maize.png" alt="maize.png"
                      width="269" height="269"
                      data-api-endpoint="https://usu.instructure.com/api/v1/users/1298792/files/81034622"
                      data-api-returntype="File" data-id="81034622" style="max-width: 269px;">&nbsp;&nbsp;</p>
                  <p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Maize (Corn)</p>
                  <p>&nbsp;</p>
                  <p><span style="text-decoration: underline;"><strong><span style="font-size: 18pt;">Conclusion &amp;
                          Summary</span></strong></span></p>
                  <p><strong>Classification</strong></p>
                  <p>Plant images present a number of unique challenges in producing a valuable and accurate classifier.
                    These challenges include:</p>
                  <ul>
                    <li>Dataset training size</li>
                    <li>Region/application specialization</li>
                    <li>Plant similarities</li>
                    <li>Robust predictions from lower-quality images</li>
                  </ul>
                  <p><span style="text-decoration: underline;">Training size</span></p>
                  <p>A valuable and useful model would require a training library several times larger than that present
                    in this project. This is due to the issue of overfitting in a relatively low number of epochs. To
                    produce a more generic plant classifier, more data would be required than in a specialized
                    classifier, and more data will require more storage handling, greater training time, and more
                    efficient data handling. All together, it is viable to create a general plant classifier, even to
                    more classes than was demonstrated here, with the caveat that potentially magnitudes larger training
                    data sets (in terms of number of images) would be required</p>
                  <p><span style="text-decoration: underline;">Specialization</span></p>
                  <p>As mentioned, specialization would potentially permit a smaller training set to generate a
                    practical and accurate classifier in a short period of time. Specialization could include manually
                    determining specific plants to focus on for a field type (say corn vs wheat fields) and local weeds
                    or pests. The classifier could be trained to handle the specific types, which decreases the risk of
                    confusion. These specialized classifiers could easily be programatically switched for handling
                    differences between growing season, year-to-year crop cycling, and other equipment reuse as
                    specified by the user or farmer.</p>
                  <p><span style="text-decoration: underline;">Similarities</span></p>
                  <p>Many plants are similar in nature to one another as far as defining structures are concerned. For
                    example, as seen above wheat and grasses or grass-like plants are quite similar in most cases. Only
                    one of those would ever be desired as a crop, Wheat, which was falsely predicted as Black-grass 100%
                    of the time in the small test performed. This would be unacceptable in a production or end user
                    environment, but could be rectified by the use of more training data to differentiate the
                    similarities, specialization of classifiers (if a similar plant type will never or rarely be
                    present, don't try to handle classifying it), or training with more diverse data such as differently
                    lighted images, more varied growth stages, and/or higher detailed images.</p>
                  <p><span style="text-decoration: underline;">Robust predictions</span></p>
                  <p>The likely application of this classifier is with a live-fed stream of data from a tractor mounted
                    camera system as would be found in a Precision Agriculture setup. These images may not be well
                    focused, or with variable lighting between images and plant quantities or clarity present for
                    various reasons. A classification model could be built, guarded against overfitting and with
                    sufficient, varied training data, to handle lower-quality images and still provide a reasonable
                    prediction accuracy.</p>
                  <p>&nbsp;</p>
                  <p><strong>Thus in conclusion, </strong>it is practical and possible to create a reliable plant
                    classifier for use in precision agriculture applications. To do so effectively one must take into
                    account the issues of training data available, specialization requirements per application,
                    knowledge of similarly-shaped plant species which may be present, and the methods by which the data
                    will be collected. If these factors are considered carefully when determining a model to train, it
                    is very reasonable a highly accurate plant classifier could be produced and implemented in
                    production-grade agriculture systems.</p>
                  <p>&nbsp;</p>
                  <p><span
                      style="text-decoration: underline; font-size: 18pt;"><strong>Future&nbsp;</strong></span><span
                      style="font-size: 24px;"><strong><u>Progress</u></strong></span></p>
                  <p>Future work would be to establish the following items for true usability in a Precision Ag
                    applications:</p>
                  <p><strong>Object Detection</strong></p>
                  <p>To identify where and define all the plants in an image as fed to the software by a camera mount,
                    an object detector model would need to be used. This model requires a higher rate of speed over
                    near-perfect accuracy, and as such could make use of the Keras RetinaNet single stage detector. This
                    would permit real-time use on machinery. The object detection process would also provide the
                    relative location of plants which could be mapped to a mechanical armature movement allowing for the
                    actual mechanical weeding or spot spraying to occur.</p>
                  <p><strong>New Class Integration</strong></p>
                  <p>To best make use of specialization practices, some interface or efficient methodology for providing
                    data with appropriately labelled classes should be established. New crop types, weeds (such as
                    dandelions) and more should be easily integrated into a new model for training and classifier
                    producing.</p>
                  <p><strong>Notes on images</strong></p>
                  <p>Image sources for classification training should contain a single to no more than 3 of the same
                    plant in the image. A common issue present in many datasets was the absence of isolated plants,
                    which would easily falsely train a classifier to recognize some plants as others. This was most
                    common in the dandelion sources found. Dandelions are a very common weed and would be easily
                    recognizable, but the primary large datasets available were comprised mainly of dandelions embedded
                    in grasses or leaves, which is strikingly similar to many of the narrow-leafed plant types already
                    trained as shown in this project. Note that in an average cropland, some conditions like thick grass
                    everywhere are extremely rare or nonexistent situations. Below is an example of the average
                    dandelion image from the best currently available datasets.</p>
                  <p><img src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/IMG_1149.jpg"
                      alt="IMG_1149.jpg" width="403" height="403"
                      data-api-endpoint="https://usu.instructure.com/api/v1/users/1298792/files/81053896"
                      data-api-returntype="File" data-id="81053896" style="max-width: 403px;">&nbsp;&nbsp;</p>
                  <p>&nbsp;</p>
                  <p><span style="text-decoration: underline; font-size: 18pt;"><strong>Other Components</strong></span>
                  </p>
                  <p>Research:</p>
                  <ul>
                    <li><a href="https://www.nature.com/articles/s41598-018-38343-3" target="_blank" class="external"
                        rel="noreferrer noopener"><span>https://www.nature.com/articles/s41598-018-38343-3</span><span
                          aria-hidden="true" class="ui-icon ui-icon-extlink ui-icon-inline"
                          title="Links to an external site."></span><span class="screenreader-only">&nbsp;(Links to an
                          external site.)</span></a></li>
                    <li><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7305380/" target="_blank" class="external"
                        rel="noreferrer noopener"><span>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7305380/</span><span
                          aria-hidden="true" class="ui-icon ui-icon-extlink ui-icon-inline"
                          title="Links to an external site."></span><span class="screenreader-only">&nbsp;(Links to an
                          external site.)</span></a></li>
                    <li><a href="https://www.sciencedirect.com/science/article/pii/S0168169920312709" target="_blank"
                        class="external"
                        rel="noreferrer noopener"><span>https://www.sciencedirect.com/science/article/pii/S0168169920312709</span><span
                          aria-hidden="true" class="ui-icon ui-icon-extlink ui-icon-inline"
                          title="Links to an external site."></span><span class="screenreader-only">&nbsp;(Links to an
                          external site.)</span></a></li>
                    <li><a
                        href="https://www.researchgate.net/publication/326211032_Machine_Vision_Retrofit_System_for_Mechanical_Weed_Control_in_Precision_Agriculture_Applications"
                        target="_blank" class="external"
                        rel="noreferrer noopener"><span>https://www.researchgate.net/publication/326211032_Machine_Vision_Retrofit_System_for_Mechanical_Weed_Control_in_Precision_Agriculture_Applications</span><span
                          aria-hidden="true" class="ui-icon ui-icon-extlink ui-icon-inline"
                          title="Links to an external site."></span><span class="screenreader-only">&nbsp;(Links to an
                          external site.)</span></a></li>
                  </ul>
                  <p><span style="text-decoration: underline;"><span
                        style="font-size: 14pt;">Tutorials/Training:</span></span></p>
                  <ul>
                    <li><a href="https://www.simplilearn.com/tutorials/deep-learning-tutorial/neural-network"
                        target="_blank" class="external"
                        rel="noreferrer noopener"><span>https://www.simplilearn.com/tutorials/deep-learning-tutorial/neural-network</span><span
                          aria-hidden="true" class="ui-icon ui-icon-extlink ui-icon-inline"
                          title="Links to an external site."></span><span class="screenreader-only">&nbsp;(Links to an
                          external site.)</span></a></li>
                    <li><a
                        href="https://learnopencv.com/image-classification-using-convolutional-neural-networks-in-keras/"
                        target="_blank" class="external"
                        rel="noreferrer noopener"><span>https://learnopencv.com/image-classification-using-convolutional-neural-networks-in-keras/</span><span
                          aria-hidden="true" class="ui-icon ui-icon-extlink ui-icon-inline"
                          title="Links to an external site."></span><span class="screenreader-only">&nbsp;(Links to an
                          external site.)</span></a></li>
                    <li><a href="https://stackabuse.com/image-recognition-in-python-with-tensorflow-and-keras/"
                        target="_blank" class="external"
                        rel="noreferrer noopener"><span>https://stackabuse.com/image-recognition-in-python-with-tensorflow-and-keras/</span><span
                          aria-hidden="true" class="ui-icon ui-icon-extlink ui-icon-inline"
                          title="Links to an external site."></span><span class="screenreader-only">&nbsp;(Links to an
                          external site.)</span></a></li>
                    <li><a
                        href="https://www.codespeedy.com/how-to-choose-number-of-epochs-to-train-a-neural-network-in-keras/"
                        target="_blank" class="external"
                        rel="noreferrer noopener"><span>https://www.codespeedy.com/how-to-choose-number-of-epochs-to-train-a-neural-network-in-keras/</span><span
                          aria-hidden="true" class="ui-icon ui-icon-extlink ui-icon-inline"
                          title="Links to an external site."></span><span class="screenreader-only">&nbsp;(Links to an
                          external site.)</span></a></li>
                  </ul>
                  <p>&nbsp;</p>
                  <p><span style="font-size: 18pt;"><strong>Presentations</strong></span></p>
                  <p><span style="text-decoration: underline;"><span style="font-size: 12pt;">Final Presentation and
                        Results</span></span></p>
                  <p><iframe title="embedded content"
                      src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/embed.html" width="960"
                      height="569" allowfullscreen="allowfullscreen" webkitallowfullscreen="webkitallowfullscreen"
                      mozallowfullscreen="mozallowfullscreen"></iframe>&nbsp;</p>
                  <p><span style="text-decoration: underline;"><span style="font-size: 12pt;">Project Midpoint
                        Update</span></span></p>
                  <p><iframe title="embedded content"
                      src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/embed(1).html" width="960"
                      height="569" allowfullscreen="allowfullscreen" webkitallowfullscreen="webkitallowfullscreen"
                      mozallowfullscreen="mozallowfullscreen"></iframe>&nbsp;</p>
                  <p><span style="text-decoration: underline;"><span style="font-size: 12pt;">Project
                        Overview</span></span></p>
                  <p><span class="instructure_file_holder link_holder"><a class="file_preview_link"
                        title="Project In Short-2.pptx"
                        href="https://usu.instructure.com/users/1298792/files/80452806?wrap=1&amp;verifier=hFxK2kcHs6xBpxw4nZlDufkh9FoQ7aWwONcQSrxf"
                        target="_blank" data-canvas-previewable="false"
                        data-api-endpoint="https://usu.instructure.com/api/v1/users/1298792/files/80452806"
                        data-api-returntype="File" aria-expanded="false" aria-controls="preview_1"
                        data-id="80452806">Project In Short.pptx</a><a class="file_download_btn" role="button"
                        download="" style="margin-inline-start: 5px; text-decoration: none;"
                        href="https://usu.instructure.com/users/1298792/files/80452806/download?verifier=hFxK2kcHs6xBpxw4nZlDufkh9FoQ7aWwONcQSrxf&amp;download_frd=1"
                        data-id="80452806">
                        <img style="width:16px; height:16px"
                          src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/svg_icon_download.svg"
                          alt="" role="presentation">
                        <span class="screenreader-only">
                          download
                        </span>
                      </a>
                      <div role="region" id="preview_1" style="display: none;"></div>
                    </span>&nbsp;&nbsp;</p>
                  <p><span style="text-decoration: underline;"><span style="font-size: 12pt;">Concept
                        Background</span></span></p>
                  <p>&nbsp; <span class="instructure_file_holder link_holder"><a class="file_preview_link"
                        title="Presentation_Precision_AG.pptx"
                        href="https://usu.instructure.com/users/1298792/files/80084081?wrap=1&amp;verifier=UDi3MmOijXQhLcSZByaXlUFCbRUruyqDB8yVlI3E"
                        target="_blank" data-canvas-previewable="false"
                        data-api-endpoint="https://usu.instructure.com/api/v1/users/1298792/files/80084081"
                        data-api-returntype="File" aria-expanded="false" aria-controls="preview_2"
                        data-id="80084081">Presentation_Precision_AG.pptx</a><a class="file_download_btn" role="button"
                        download="" style="margin-inline-start: 5px; text-decoration: none;"
                        href="https://usu.instructure.com/users/1298792/files/80084081/download?verifier=UDi3MmOijXQhLcSZByaXlUFCbRUruyqDB8yVlI3E&amp;download_frd=1"
                        data-id="80084081">
                        <img style="width:16px; height:16px"
                          src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/svg_icon_download.svg"
                          alt="" role="presentation">
                        <span class="screenreader-only">
                          download
                        </span>
                      </a>
                      <div role="region" id="preview_2" style="display: none;"></div>
                    </span>&nbsp;&nbsp;</p>
                  <p>&nbsp;</p>
                  <p><span style="font-size: 18pt;"><strong>About Me</strong></span></p>
                  <table style="border-collapse: collapse; width: 98%; height: 294px; border-style: hidden;" border="0"
                    cellpadding="5">
                    <tbody>
                      <tr style="height: 294px;">
                        <td style="width: 49.9072%; height: 294px;">
                          <p>Currently a senior pursuing a CS Bachelorâ€™s degree</p>
                          <p>Machine learning experience:</p>
                          <ul>
                            <li>Advanced Algorithms, limited supplemental reading</li>
                          </ul>
                          <p>Experience/Skills:</p>
                          <ul>
                            <li>git (Github, bitbucket, etc), Jira, remote work tools (slack, teams, discord, zoom, etc)
                            </li>
                            <li>Python, Java, javascript, some web/full-stack, SQL and NoSQL DB</li>
                          </ul>
                          <p>Interests and History:</p>
                          <ul>
                            <li>Worked 6 years in Agriculture research/GMO products for biofuels, food products, and
                              livestock feed</li>
                            <li>Originally from IA, ~40-45% of electricity production is wind</li>
                            <li>Basic familiarity with home geothermal system for HVAC</li>
                            <li>Interest in smaller-scale power systems (local and home power grids/systems)</li>
                          </ul>
                        </td>
                        <td style="width: 50%; height: 294px;"><img
                            src="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/piano_portrait.jpg"
                            alt="piano_portrait.jpg" width="217" height="290"
                            data-api-endpoint="https://usu.instructure.com/api/v1/users/1298792/files/80083943"
                            data-api-returntype="File" data-id="80083943" style="max-width: 217px;"></td>
                      </tr>
                    </tbody>
                  </table>
                  <p>&nbsp;</p>
                  <h4>Discussion Board for Comments:</h4>
                  <p><a title="Jake Cook - Plant ID with CNN&#39;s"
                      href="https://usu.instructure.com/courses/639034/discussion_topics/2264163"
                      data-api-endpoint="https://usu.instructure.com/api/v1/courses/639034/discussion_topics/2264163"
                      data-api-returntype="Discussion">Jake Cook - Plant ID with CNN's</a>&nbsp;</p>

                </div>
              </div>
              <div id="module_navigation_target">
                <div style="display: none;"></div>
              </div>

            </div>
          </div>
          <div id="right-side-wrapper" class="ic-app-main-content__secondary">
            <aside id="right-side" role="complementary">

            </aside>
          </div>
        </div>
      </div>
    </div>



    <div style="display:none;">
      <!-- Everything inside of this should always stay hidden -->
      <div id="page_view_id">61ae14f5-dbd3-44dd-a25e-0ac9398be269</div>
    </div>
    <div id="aria_alerts" class="hide-text affix" role="alert" aria-live="assertive"></div>
    <div id="StudentTray__Container"></div>


    <script>
      Object.assign(
        ENV,
        {}
      )
    </script>

    <link rel="preload"
      href="./Jake Cook&#39;s Portfolio_ Spring 2021 CS-6620-LW1 XL_files/inst_fs_service_worker-c-a44e3dc046.js"
      as="script" type="text/javascript">
    <script>
        //<![CDATA[
        (window.bundles || (window.bundles = [])).push('inst_fs_service_worker');
//]]>
    </script>
    <script>
      //<![CDATA[

      ;["https://instructure-uploads-2.s3.amazonaws.com/account_10090000000000015/attachments/79630621/canvas_global_redirect.js"].forEach(function (src) {
        var s = document.createElement('script')
        s.src = src
        s.async = false
        document.head.appendChild(s)
      });
//]]>
    </script>

  </div> <!-- #application -->


  <div id="nav-tray-portal" style="position: relative; z-index: 99;"></div>
  <script type="text/javascript" id="">var courseName, term, college, dept, deliver, campus;
    if (-1 < document.location.href.indexOf("/courses/")) {
      var localStorageCourseVar = "courseInfo_" + ENV.COURSE_ID; if (null === localStorage.getItem(localStorageCourseVar)) $.getJSON("https://elearn.usu.edu/canvasCustomTools/course_tracking/course_details_ajax.php?id\x3d" + ENV.COURSE_ID, function (a) { localStorage.setItem(localStorageCourseVar, JSON.stringify(a)); courseName = a.name; term = a.term; college = a.college; dept = a.dept; deliver = a.delivery; campus = a.campus }); else {
        var jsonObject = jQuery.parseJSON(localStorage.getItem(localStorageCourseVar));
        courseName = jsonObject.name; term = jsonObject.term; college = jsonObject.college; dept = jsonObject.dept; deliver = jsonObject.delivery; campus = jsonObject.campus
      }
    };</script>
  <div role="log" aria-live="assertive" aria-relevant="additions" class="ally-helper-hidden-accessible"></div>
</body>

</html>